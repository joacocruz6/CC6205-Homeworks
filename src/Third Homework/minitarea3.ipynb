{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "colab_type": "text",
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "# Minitarea 3\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "Nombre: Joaquin Cruz\n",
    "\n",
    "Fecha de Entrega: Domingo 17 de Mayo\n",
    "\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "- El ejercicio consiste en:\n",
    "\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    \n",
    "    - Entrenar Word2Vec y FastText sobre un pequeño corpus.\n",
    "    \n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "\n",
    "- La minitarea es INDIVIDUAL.\n",
    "\n",
    "- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n",
    "\n",
    "- La entrega debe ser por u-cursos.\n",
    "\n",
    "- Atrasos: se descontará un punto por día hábil de atraso tanto para las mini-tareas como para las competencias.\n",
    "\n",
    "- En el horario de auxiliar se abrirán horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n",
    "\n",
    "- Cada sección tiene un punto base y se evalúa sobre 6 puntos.\n",
    "\n",
    "- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores.\n",
    "\n",
    "\n",
    "## Referencias   \n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## Preguntas Teóricas\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### Parte 1: Modelos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. (3 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Representación escogida del documento de entrada: \n",
    "\n",
    "Como estamos viendo pertenencia a clases, puede ser util un modelo BOW simple, pero puede ser muy limitado y el modelo se puede portar mal sobre palabras que esten en la 'intersección' de las clases (por ejemplo si un documento dice ministra del deporte, y esta etiquetado como politica pues fue un cambio de gabinete el modelo puede llegar a portarse mal con respecto a ese tipo de articulos). Por tanto un añadir una feature al modelo sobre los trigramas puede portarse mucho mejor pues puede capturar un poco más el contexto de las palabras y asi poder descriminar mejor entre los documentos (el de bigramas tambien podria servir, pero eso seria mas util cuando hay analisis de sentimientos involucrado y aqui no nos afectan tanto las negaciones, o pares de palabras pueden tambien producir el efecto anterior descrito). Asi la representación escogida seria BOW con features de trigramas.\n",
    "\n",
    "\n",
    "Parámetros del modelo:\n",
    "\n",
    "Los parametros del modelo a definir son los pesos de la matriz $W$ que son representados en el modelo lineal $\\vec{\\hat{y}} = \\vec{x}\\cdot W + \\vec{b}, en este caso estos representan el peso que le atribuye el modelo a las palabras y los trigramas en de los articulos con cada clase.\n",
    "\n",
    "\n",
    "Transformaciones necesarias:\n",
    "\n",
    "Hay que aplicar la funcion sigmoide al output de cada predicción, pues esta funcion mapea los resultados del modelo lineal a el rango $[0,1]$ y nos entrega una distribucion de probabilidad en el vector de salida del modelo que seria ''el grado de pertenencia a cada clase''\n",
    "\n",
    "\n",
    "Función de pérdida escogida:\n",
    "\n",
    "Como se pasa por una transformacion de softmax, y el problema es multiclase entonces seria mejor escoger una Categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique el proceso de entrenamiento y evaluación del modelo. (3 puntos)\n",
    "\n",
    "**Respuesta**: El modelo entrena bajo el algoritmo de descenso estocastico. En este se inician todos los parametros de la matriz $W$, llamemosles $\\theta_{i}$, consideremos $\\vec{y}$ como los resultados reales de las distribuciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### Parte 2: Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![Red](https://drive.google.com/uc?id=1Yd0s9g5SlB1-XuVokGQO2J-yDudQe2Kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, represéntela matemáticamente, entregando las dimensiones de las matrices y vectores. (3 Puntos)\n",
    "\n",
    "**Respuesta**: Considerando un bias $b^{i}$ para la $i$-esima capa, con $i \\in {1,2,3,4}$, tenemos que la descripcion matematica en cada capa es la evaluacion del modelo lineal pasado por la funcion de activación. Por tanto desde que el input \"entra\" en la red hasta que sale son aplicadas distintas transformaciones lineales en cada capa como sigue:\n",
    "\n",
    "Utilizando la notacion de que $g(\\vec{x}) = (g(x_{1}),g(x_{2}),...,g(x_{n}))$ con $n = dim(\\vec{x})$ al utilizar las funciones de activacion que son $g,f,h: \\mathbb{R} \\to \\mathbb{R}$\n",
    "\n",
    "$\\vec{h^{1}} = \\vec{x} \\cdot W^{1} + \\vec{b^{1}}$ \n",
    "\n",
    "con $dim(\\vec{x}) = 3;\\;dim(W^{1})=3\\times2;\\;dim(\\vec{b^{1}})=2$\n",
    "\n",
    "$\\vec{z^{1}} = g(\\vec{h^{1}})$\n",
    "\n",
    "con $dim(g(\\vec{h^{1}})) = 2\\;dim(\\vec{z^{1}}) = 2$\n",
    "\n",
    "$\\vec{h^{2}} = \\vec{z^{1}} \\cdot W^{2} + \\vec{b^{2}}$\n",
    "\n",
    "con $dim(\\vec{z^{1}}) = 2;\\;dim(W^{2})=2\\times3;\\;dim(\\vec{b^{2}})=3$\n",
    "\n",
    "$\\vec{z^{2}} = f(\\vec{h^{2}})$\n",
    "\n",
    "con $dim(f(\\vec{h^{2}})) = 3\\;dim(\\vec{z^{2}}) = 3$\n",
    "\n",
    "$\\vec{h^{3}} = \\vec{z^{2}} \\cdot W^{3} + \\vec{b^{3}}$\n",
    "\n",
    "con $dim(\\vec{z^{2}}) = 3;\\;dim(W^{3})=3\\times1;\\;dim(\\vec{b^{3}})=1$\n",
    "\n",
    "$\\vec{z^{3}} = h(\\vec{h^{3}})$\n",
    "\n",
    "con $dim(h(\\vec{h^{3}})) = 1\\;dim(\\vec{z^{3}}) = 1$\n",
    "\n",
    "$\\vec{\\hat{y}} = \\vec{z^{3}} \\cdot W^{4} + \\vec{b^{4}}$\n",
    "\n",
    "con $dim(\\vec{z^{3}}) = 1;\\;dim(W^{4})=1\\times4;\\;dim(\\vec{b^{4}})=4$\n",
    "\n",
    "\n",
    "Por lo tanto, reemplazando nos da que la formula es:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(f(g(\\vec{x} \\cdot W^{1} + \\vec{b^{1}})\\cdot W^{2} + \\vec{b^{2}})\\cdot W^{3} + \\vec{b^{3}})\\cdot W^{4} + \\vec{b^{4}}$\n",
    "\n",
    "Con $dim(\\vec{\\hat{y}}) = 4$\n",
    "\n",
    "**Pregunta 2**: Qué es backpropagation? Cuales serían los parámetros a evaluar en la red neuronal anterior? (1 punto)\n",
    "\n",
    "**Respuesta**: Backpropagation es un algoritmo que nos facilita el calculo del gradiente de la función de perdida cuando debemos entrenar a las redes neuronales. El objetivo del algoritmo es el calculo del gradiente de la función de perdida aplicada en la red con respecto a los parametros que esta debe estimar, que son los pesos de cada paso, es decir los $W^{i}, \\vec{b^{i}}$ utilizando de forma inteligente la regla de la cadena de las derivadas.\n",
    "\n",
    "Como se dijo anteriormente, los parametros a evaluar en la red anterior son: $W^{1},W^{2},W^{3},W^{4}$ y $\\vec{b^{1}},\\vec{b^{2}},\\vec{b^{3}},\\vec{b^{4}$\n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? (2 puntos)\n",
    "\n",
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## Pregunta Práctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### Parte 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecCvnryeQiG7"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY3kmg4onnsu"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias\n",
    "dataset = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. (4 puntos) (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**: Veamos un poco los datos que tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YILUICGtYJo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            raw_character_text  \\\n",
      "32932      C. Montgomery Burns   \n",
      "61157                    Kodos   \n",
      "52329            Homer Simpson   \n",
      "61638   Apu Nahasapeemapetilon   \n",
      "115430           Homer Simpson   \n",
      "29544                    Chase   \n",
      "68134             Lisa Simpson   \n",
      "77090             Rod Flanders   \n",
      "100936           Marge Simpson   \n",
      "59640            Homer Simpson   \n",
      "24406            Homer Simpson   \n",
      "16814             Chief Wiggum   \n",
      "98594            Homer Simpson   \n",
      "66009             Bart Simpson   \n",
      "3049             Marge Simpson   \n",
      "34260            Marge Simpson   \n",
      "54709             Bart Simpson   \n",
      "102751             Moe Szyslak   \n",
      "105063                  Viktor   \n",
      "106543        Krusty the Clown   \n",
      "\n",
      "                                             spoken_words  \n",
      "32932                                   But I'm shopping!  \n",
      "61157          Ooh, it's Burt Reynolds and Michael Jeter!  \n",
      "52329                         My uncle still has my nose.  \n",
      "61638                   then assistant to Lorne Michaels.  \n",
      "115430                                            I wish.  \n",
      "29544                             God I missed you, Lulu.  \n",
      "68134   I could join past presidents. Like Otto... or ...  \n",
      "77090                            But I have a girlfriend!  \n",
      "100936                                 That's crazy talk!  \n",
      "59640                         I'm bored. Heyyy... a fair!  \n",
      "24406            Free sample of fabric softener? Woo hoo!  \n",
      "16814        Yeah, I'll say. Magic ticket my ass, McBain.  \n",
      "98594                                            Woo hoo!  \n",
      "66009   Check it out! I'm Tomokaz Ohka of the Montreal...  \n",
      "3049    Well, it's a lot. In fact, your father had to ...  \n",
      "34260                         Did you ever tell him that?  \n",
      "54709                                 I've got a partner.  \n",
      "102751  I'm tellin' ya, I'm fine. I've never been happ...  \n",
      "105063                                            Is him!  \n",
      "106543                 I don't know if I've got it in me.  \n",
      "(131853, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.sample(20))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos unir los personajes con las palabras que ellos hablan, de esta forma podemos entrenar nuestros embeddings relacionando la frase que dice cada personaje con lo que este habla, debemos poner ojo pues puntuaciones de `string.punctuation` incluyen '-' o \"'\" que son usadas en expresiones recurrentes en los simpsons (por ejemplo \"D'oh\" de Homero Simpson). Pero esto se puede remediar al agregar pares de palabras mas adelante que tengan un mejor significado (asi como lo se hizo con `nueva york` en el auxiliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset[\"raw_character_text\"] + \" \" + dataset[\"spoken_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "print(punctuation)\n",
    "word_tokens = [simple_tokenizer(doc) for doc in content.values]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lisa', 'Simpson', 'Wheres', 'Mr', 'Bergstrom']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora juntamos frases que puedan ser vistas como una a partir de como se distribuyen en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-07 23:24:56,951 : INFO : collecting all words and their counts\n",
      "2020-06-07 23:24:56,954 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-06-07 23:24:57,142 : INFO : PROGRESS: at sentence #5000, processed 58962 words and 38714 word types\n",
      "2020-06-07 23:24:57,289 : INFO : PROGRESS: at sentence #10000, processed 116303 words and 67474 word types\n",
      "2020-06-07 23:24:57,430 : INFO : PROGRESS: at sentence #15000, processed 171261 words and 92181 word types\n",
      "2020-06-07 23:24:57,583 : INFO : PROGRESS: at sentence #20000, processed 234725 words and 118440 word types\n",
      "2020-06-07 23:24:57,741 : INFO : PROGRESS: at sentence #25000, processed 297388 words and 143880 word types\n",
      "2020-06-07 23:24:57,916 : INFO : PROGRESS: at sentence #30000, processed 363426 words and 169356 word types\n",
      "2020-06-07 23:24:58,153 : INFO : PROGRESS: at sentence #35000, processed 426138 words and 191490 word types\n",
      "2020-06-07 23:24:58,338 : INFO : PROGRESS: at sentence #40000, processed 483260 words and 210723 word types\n",
      "2020-06-07 23:24:58,807 : INFO : PROGRESS: at sentence #45000, processed 539593 words and 229784 word types\n",
      "2020-06-07 23:24:59,013 : INFO : PROGRESS: at sentence #50000, processed 594804 words and 248360 word types\n",
      "2020-06-07 23:24:59,319 : INFO : PROGRESS: at sentence #55000, processed 648793 words and 265884 word types\n",
      "2020-06-07 23:24:59,474 : INFO : PROGRESS: at sentence #60000, processed 700367 words and 281851 word types\n",
      "2020-06-07 23:24:59,634 : INFO : PROGRESS: at sentence #65000, processed 755232 words and 298821 word types\n",
      "2020-06-07 23:24:59,790 : INFO : PROGRESS: at sentence #70000, processed 816341 words and 318657 word types\n",
      "2020-06-07 23:24:59,948 : INFO : PROGRESS: at sentence #75000, processed 877055 words and 337366 word types\n",
      "2020-06-07 23:25:00,139 : INFO : PROGRESS: at sentence #80000, processed 938145 words and 355645 word types\n",
      "2020-06-07 23:25:00,311 : INFO : PROGRESS: at sentence #85000, processed 998833 words and 373741 word types\n",
      "2020-06-07 23:25:00,466 : INFO : PROGRESS: at sentence #90000, processed 1057552 words and 391071 word types\n",
      "2020-06-07 23:25:00,619 : INFO : PROGRESS: at sentence #95000, processed 1118175 words and 408042 word types\n",
      "2020-06-07 23:25:00,765 : INFO : PROGRESS: at sentence #100000, processed 1177810 words and 425339 word types\n",
      "2020-06-07 23:25:00,933 : INFO : PROGRESS: at sentence #105000, processed 1238652 words and 442481 word types\n",
      "2020-06-07 23:25:01,083 : INFO : PROGRESS: at sentence #110000, processed 1300175 words and 460051 word types\n",
      "2020-06-07 23:25:01,227 : INFO : PROGRESS: at sentence #115000, processed 1359523 words and 476101 word types\n",
      "2020-06-07 23:25:01,380 : INFO : PROGRESS: at sentence #120000, processed 1419933 words and 492590 word types\n",
      "2020-06-07 23:25:01,524 : INFO : PROGRESS: at sentence #125000, processed 1479504 words and 507885 word types\n",
      "2020-06-07 23:25:01,690 : INFO : PROGRESS: at sentence #130000, processed 1538844 words and 521438 word types\n",
      "2020-06-07 23:25:01,768 : INFO : collected 526421 word types from a corpus of 1560756 words (unigram + bigrams) and 131853 sentences\n",
      "2020-06-07 23:25:01,769 : INFO : using 526421 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'Miss': 374,\n",
       "             b'Hoover': 206,\n",
       "             b'Miss_Hoover': 198,\n",
       "             b'No': 3855,\n",
       "             b'Hoover_No': 5,\n",
       "             b'actually': 267,\n",
       "             b'No_actually': 6,\n",
       "             b'it': 12164,\n",
       "             b'actually_it': 4,\n",
       "             b'was': 5334,\n",
       "             b'it_was': 500,\n",
       "             b'a': 28723,\n",
       "             b'was_a': 607,\n",
       "             b'little': 2284,\n",
       "             b'a_little': 1107,\n",
       "             b'of': 15630,\n",
       "             b'little_of': 12,\n",
       "             b'both': 280,\n",
       "             b'of_both': 5,\n",
       "             b'Sometimes': 68,\n",
       "             b'both_Sometimes': 1,\n",
       "             b'when': 1746,\n",
       "             b'Sometimes_when': 3,\n",
       "             b'when_a': 36,\n",
       "             b'disease': 33,\n",
       "             b'a_disease': 2,\n",
       "             b'is': 11409,\n",
       "             b'disease_is': 2,\n",
       "             b'in': 11719,\n",
       "             b'is_in': 147,\n",
       "             b'all': 4882,\n",
       "             b'in_all': 31,\n",
       "             b'the': 38582,\n",
       "             b'all_the': 740,\n",
       "             b'magazines': 25,\n",
       "             b'the_magazines': 2,\n",
       "             b'and': 13527,\n",
       "             b'magazines_and': 1,\n",
       "             b'and_all': 76,\n",
       "             b'news': 259,\n",
       "             b'the_news': 29,\n",
       "             b'shows': 129,\n",
       "             b'news_shows': 1,\n",
       "             b'its': 3334,\n",
       "             b'shows_its': 3,\n",
       "             b'only': 1498,\n",
       "             b'its_only': 26,\n",
       "             b'natural': 68,\n",
       "             b'only_natural': 4,\n",
       "             b'that': 11055,\n",
       "             b'natural_that': 1,\n",
       "             b'you': 30228,\n",
       "             b'that_you': 239,\n",
       "             b'think': 2700,\n",
       "             b'you_think': 486,\n",
       "             b'think_you': 207,\n",
       "             b'have': 7191,\n",
       "             b'you_have': 738,\n",
       "             b'have_it': 84,\n",
       "             b'Lisa': 12891,\n",
       "             b'Simpson': 68843,\n",
       "             b'Lisa_Simpson': 10919,\n",
       "             b'Wheres': 254,\n",
       "             b'Simpson_Wheres': 78,\n",
       "             b'Mr': 1759,\n",
       "             b'Wheres_Mr': 2,\n",
       "             b'Bergstrom': 19,\n",
       "             b'Mr_Bergstrom': 19,\n",
       "             b'I': 35713,\n",
       "             b'Hoover_I': 11,\n",
       "             b'dont': 5235,\n",
       "             b'I_dont': 2277,\n",
       "             b'know': 4633,\n",
       "             b'dont_know': 772,\n",
       "             b'Although': 74,\n",
       "             b'know_Although': 1,\n",
       "             b'Id': 1086,\n",
       "             b'Although_Id': 1,\n",
       "             b'sure': 1127,\n",
       "             b'Id_sure': 6,\n",
       "             b'like': 5536,\n",
       "             b'sure_like': 5,\n",
       "             b'to': 26349,\n",
       "             b'like_to': 672,\n",
       "             b'talk': 593,\n",
       "             b'to_talk': 159,\n",
       "             b'talk_to': 150,\n",
       "             b'him': 2342,\n",
       "             b'to_him': 99,\n",
       "             b'He': 1057,\n",
       "             b'him_He': 10,\n",
       "             b'didnt': 1331,\n",
       "             b'He_didnt': 22,\n",
       "             b'touch': 129,\n",
       "             b'didnt_touch': 5,\n",
       "             b'my': 10680,\n",
       "             b'touch_my': 8,\n",
       "             b'lesson': 126,\n",
       "             b'my_lesson': 11,\n",
       "             b'plan': 203,\n",
       "             b'lesson_plan': 2,\n",
       "             b'What': 3551,\n",
       "             b'plan_What': 1,\n",
       "             b'did': 1922,\n",
       "             b'What_did': 88,\n",
       "             b'he': 2443,\n",
       "             b'did_he': 44,\n",
       "             b'teach': 185,\n",
       "             b'he_teach': 1,\n",
       "             b'teach_you': 59,\n",
       "             b'That': 1459,\n",
       "             b'Simpson_That': 467,\n",
       "             b'life': 1081,\n",
       "             b'That_life': 1,\n",
       "             b'life_is': 51,\n",
       "             b'worth': 140,\n",
       "             b'is_worth': 15,\n",
       "             b'living': 176,\n",
       "             b'worth_living': 5,\n",
       "             b'Edna': 861,\n",
       "             b'KrabappelFlanders': 719,\n",
       "             b'Edna_KrabappelFlanders': 719,\n",
       "             b'The': 4193,\n",
       "             b'KrabappelFlanders_The': 6,\n",
       "             b'polls': 13,\n",
       "             b'The_polls': 1,\n",
       "             b'will': 2550,\n",
       "             b'polls_will': 1,\n",
       "             b'be': 6853,\n",
       "             b'will_be': 515,\n",
       "             b'open': 299,\n",
       "             b'be_open': 3,\n",
       "             b'from': 2794,\n",
       "             b'open_from': 1,\n",
       "             b'now': 2859,\n",
       "             b'from_now': 62,\n",
       "             b'until': 280,\n",
       "             b'now_until': 2,\n",
       "             b'until_the': 30,\n",
       "             b'end': 339,\n",
       "             b'the_end': 149,\n",
       "             b'end_of': 108,\n",
       "             b'recess': 12,\n",
       "             b'of_recess': 1,\n",
       "             b'Now': 3036,\n",
       "             b'recess_Now': 1,\n",
       "             b'just': 5500,\n",
       "             b'Now_just': 25,\n",
       "             b'just_in': 41,\n",
       "             b'case': 193,\n",
       "             b'in_case': 33,\n",
       "             b'any': 1076,\n",
       "             b'case_any': 1,\n",
       "             b'any_of': 97,\n",
       "             b'of_you': 502,\n",
       "             b'decided': 84,\n",
       "             b'have_decided': 9,\n",
       "             b'decided_to': 54,\n",
       "             b'put': 870,\n",
       "             b'to_put': 124,\n",
       "             b'put_any': 2,\n",
       "             b'thought': 932,\n",
       "             b'any_thought': 1,\n",
       "             b'into': 1160,\n",
       "             b'thought_into': 4,\n",
       "             b'this': 9064,\n",
       "             b'into_this': 49,\n",
       "             b'well': 1492,\n",
       "             b'this_well': 6,\n",
       "             b'well_have': 54,\n",
       "             b'our': 2970,\n",
       "             b'have_our': 16,\n",
       "             b'final': 94,\n",
       "             b'our_final': 12,\n",
       "             b'statements': 7,\n",
       "             b'final_statements': 1,\n",
       "             b'Martin': 519,\n",
       "             b'statements_Martin': 1,\n",
       "             b'Prince': 436,\n",
       "             b'Martin_Prince': 409,\n",
       "             b'Prince_I': 26,\n",
       "             b'dont_think': 247,\n",
       "             b'theres': 1021,\n",
       "             b'think_theres': 26,\n",
       "             b'anything': 764,\n",
       "             b'theres_anything': 10,\n",
       "             b'left': 530,\n",
       "             b'anything_left': 2,\n",
       "             b'left_to': 24,\n",
       "             b'say': 1944,\n",
       "             b'to_say': 358,\n",
       "             b'Bart': 16499,\n",
       "             b'KrabappelFlanders_Bart': 40,\n",
       "             b'Bart_Simpson': 13243,\n",
       "             b'Victory': 7,\n",
       "             b'Simpson_Victory': 4,\n",
       "             b'party': 349,\n",
       "             b'Victory_party': 1,\n",
       "             b'under': 322,\n",
       "             b'party_under': 1,\n",
       "             b'under_the': 82,\n",
       "             b'slide': 31,\n",
       "             b'the_slide': 5,\n",
       "             b'Simpson_Mr': 155,\n",
       "             b'Bergstrom_Mr': 1,\n",
       "             b'Landlady': 3,\n",
       "             b'Hey': 3901,\n",
       "             b'Landlady_Hey': 1,\n",
       "             b'hey': 349,\n",
       "             b'Hey_hey': 175,\n",
       "             b'hey_he': 1,\n",
       "             b'Moved': 2,\n",
       "             b'he_Moved': 1,\n",
       "             b'out': 4258,\n",
       "             b'Moved_out': 1,\n",
       "             b'out_this': 32,\n",
       "             b'morning': 292,\n",
       "             b'this_morning': 42,\n",
       "             b'morning_He': 1,\n",
       "             b'must': 774,\n",
       "             b'He_must': 18,\n",
       "             b'must_have': 109,\n",
       "             b'have_a': 1216,\n",
       "             b'new': 1172,\n",
       "             b'a_new': 344,\n",
       "             b'job': 568,\n",
       "             b'new_job': 16,\n",
       "             b'job_he': 2,\n",
       "             b'took': 324,\n",
       "             b'he_took': 8,\n",
       "             b'his': 2019,\n",
       "             b'took_his': 5,\n",
       "             b'Copernicus': 5,\n",
       "             b'his_Copernicus': 1,\n",
       "             b'costume': 50,\n",
       "             b'Copernicus_costume': 1,\n",
       "             b'Do': 784,\n",
       "             b'Simpson_Do': 202,\n",
       "             b'Do_you': 467,\n",
       "             b'you_know': 857,\n",
       "             b'where': 1114,\n",
       "             b'know_where': 86,\n",
       "             b'where_I': 111,\n",
       "             b'could': 2147,\n",
       "             b'I_could': 543,\n",
       "             b'find': 815,\n",
       "             b'could_find': 19,\n",
       "             b'find_him': 29,\n",
       "             b'Landlady_I': 1,\n",
       "             b'I_think': 1138,\n",
       "             b'hes': 1004,\n",
       "             b'think_hes': 46,\n",
       "             b'taking': 309,\n",
       "             b'hes_taking': 3,\n",
       "             b'taking_the': 25,\n",
       "             b'next': 644,\n",
       "             b'the_next': 186,\n",
       "             b'train': 88,\n",
       "             b'next_train': 2,\n",
       "             b'train_to': 7,\n",
       "             b'Capital': 49,\n",
       "             b'to_Capital': 7,\n",
       "             b'City': 145,\n",
       "             b'Capital_City': 40,\n",
       "             b'Simpson_The': 709,\n",
       "             b'The_train': 4,\n",
       "             b'how': 1926,\n",
       "             b'train_how': 1,\n",
       "             b'how_like': 2,\n",
       "             b'like_him': 31,\n",
       "             b'traditional': 18,\n",
       "             b'him_traditional': 1,\n",
       "             b'yet': 334,\n",
       "             b'traditional_yet': 1,\n",
       "             b'environmentally': 2,\n",
       "             b'yet_environmentally': 1,\n",
       "             b'sound': 219,\n",
       "             b'environmentally_sound': 1,\n",
       "             b'Yes': 1512,\n",
       "             b'Landlady_Yes': 1,\n",
       "             b'Yes_and': 17,\n",
       "             b'and_its': 109,\n",
       "             b'been': 1846,\n",
       "             b'its_been': 40,\n",
       "             b'been_the': 32,\n",
       "             b'backbone': 7,\n",
       "             b'the_backbone': 4,\n",
       "             b'backbone_of': 4,\n",
       "             b'of_our': 266,\n",
       "             b'country': 212,\n",
       "             b'our_country': 10,\n",
       "             b'since': 350,\n",
       "             b'country_since': 1,\n",
       "             b'Leland': 1,\n",
       "             b'since_Leland': 1,\n",
       "             b'Stanford': 6,\n",
       "             b'Leland_Stanford': 1,\n",
       "             b'drove': 44,\n",
       "             b'Stanford_drove': 1,\n",
       "             b'drove_that': 1,\n",
       "             b'golden': 30,\n",
       "             b'that_golden': 5,\n",
       "             b'spike': 4,\n",
       "             b'golden_spike': 2,\n",
       "             b'at': 4221,\n",
       "             b'spike_at': 1,\n",
       "             b'Promontory': 1,\n",
       "             b'at_Promontory': 1,\n",
       "             b'point': 290,\n",
       "             b'Promontory_point': 1,\n",
       "             b'Simpson_I': 4474,\n",
       "             b'see': 2586,\n",
       "             b'I_see': 323,\n",
       "             b'see_he': 5,\n",
       "             b'touched': 38,\n",
       "             b'he_touched': 1,\n",
       "             b'touched_you': 2,\n",
       "             b'too': 1514,\n",
       "             b'you_too': 80,\n",
       "             b'Simpson_Hey': 1641,\n",
       "             b'thanks': 278,\n",
       "             b'Hey_thanks': 17,\n",
       "             b'for': 9533,\n",
       "             b'thanks_for': 59,\n",
       "             b'your': 9407,\n",
       "             b'for_your': 277,\n",
       "             b'vote': 105,\n",
       "             b'your_vote': 4,\n",
       "             b'man': 2116,\n",
       "             b'vote_man': 1,\n",
       "             b'Nelson': 1406,\n",
       "             b'Muntz': 1210,\n",
       "             b'Nelson_Muntz': 1157,\n",
       "             b'Muntz_I': 86,\n",
       "             b'I_didnt': 550,\n",
       "             b'didnt_vote': 4,\n",
       "             b'Votings': 1,\n",
       "             b'vote_Votings': 1,\n",
       "             b'Votings_for': 1,\n",
       "             b'geeks': 8,\n",
       "             b'for_geeks': 1,\n",
       "             b'Well': 5461,\n",
       "             b'Simpson_Well': 2153,\n",
       "             b'Well_you': 183,\n",
       "             b'got': 3493,\n",
       "             b'you_got': 224,\n",
       "             b'got_that': 52,\n",
       "             b'right': 3719,\n",
       "             b'that_right': 42,\n",
       "             b'Thanks': 526,\n",
       "             b'right_Thanks': 3,\n",
       "             b'Thanks_for': 152,\n",
       "             b'girls': 245,\n",
       "             b'vote_girls': 1,\n",
       "             b'Terrisherri': 3,\n",
       "             b'We': 2270,\n",
       "             b'Terrisherri_We': 2,\n",
       "             b'forgot': 140,\n",
       "             b'We_forgot': 4,\n",
       "             b'Well_dont': 31,\n",
       "             b'sweat': 25,\n",
       "             b'dont_sweat': 2,\n",
       "             b'sweat_it': 3,\n",
       "             b'Just': 1090,\n",
       "             b'it_Just': 15,\n",
       "             b'so': 3688,\n",
       "             b'Just_so': 9,\n",
       "             b'long': 651,\n",
       "             b'so_long': 45,\n",
       "             b'as': 2523,\n",
       "             b'long_as': 124,\n",
       "             b'as_a': 400,\n",
       "             b'couple': 178,\n",
       "             b'a_couple': 114,\n",
       "             b'couple_of': 83,\n",
       "             b'people': 1329,\n",
       "             b'of_people': 75,\n",
       "             b'people_did': 2,\n",
       "             b'did_right': 1,\n",
       "             b'Milhouse': 2398,\n",
       "             b'right_Milhouse': 4,\n",
       "             b'Van': 2167,\n",
       "             b'Milhouse_Van': 1757,\n",
       "             b'Houten': 2120,\n",
       "             b'Van_Houten': 2120,\n",
       "             b'Uh': 1472,\n",
       "             b'Houten_Uh': 45,\n",
       "             b'oh': 360,\n",
       "             b'Uh_oh': 49,\n",
       "             b'Lewis': 45,\n",
       "             b'Simpson_Lewis': 1,\n",
       "             b'Somebody': 70,\n",
       "             b'Simpson_Somebody': 17,\n",
       "             b'Somebody_must': 2,\n",
       "             b'voted': 29,\n",
       "             b'have_voted': 3,\n",
       "             b'Houten_What': 44,\n",
       "             b'about': 3249,\n",
       "             b'What_about': 175,\n",
       "             b'about_you': 114,\n",
       "             b'you_Bart': 62,\n",
       "             b'Didnt': 65,\n",
       "             b'Bart_Didnt': 1,\n",
       "             b'Didnt_you': 34,\n",
       "             b'you_vote': 9,\n",
       "             b'Simpson_Uh': 542,\n",
       "             b'Wendell': 24,\n",
       "             b'Borton': 15,\n",
       "             b'Wendell_Borton': 15,\n",
       "             b'Yayyyyyyyyyyyyyy': 1,\n",
       "             b'Borton_Yayyyyyyyyyyyyyy': 1,\n",
       "             b'demand': 31,\n",
       "             b'I_demand': 19,\n",
       "             b'demand_a': 6,\n",
       "             b'recount': 3,\n",
       "             b'a_recount': 1,\n",
       "             b'One': 576,\n",
       "             b'KrabappelFlanders_One': 3,\n",
       "             b'One_for': 13,\n",
       "             b'for_Martin': 6,\n",
       "             b'two': 1329,\n",
       "             b'Martin_two': 1,\n",
       "             b'two_for': 5,\n",
       "             b'Would': 235,\n",
       "             b'Martin_Would': 1,\n",
       "             b'Would_you': 178,\n",
       "             b'you_like': 418,\n",
       "             b'another': 609,\n",
       "             b'like_another': 6,\n",
       "             b'another_recount': 1,\n",
       "             b'Simpson_No': 1670,\n",
       "             b'KrabappelFlanders_Well': 48,\n",
       "             b'Well_I': 749,\n",
       "             b'I_just': 831,\n",
       "             b'want': 2669,\n",
       "             b'just_want': 91,\n",
       "             b'want_to': 1159,\n",
       "             b'make': 1870,\n",
       "             b'to_make': 461,\n",
       "             b'make_sure': 90,\n",
       "             b'sure_One': 1,\n",
       "             b'Two': 243,\n",
       "             b'Martin_Two': 1,\n",
       "             b'Two_for': 3,\n",
       "             b'Kid': 118,\n",
       "             b'Reporter': 83,\n",
       "             b'Kid_Reporter': 1,\n",
       "             b'This': 2475,\n",
       "             b'Reporter_This': 1,\n",
       "             b'way': 1625,\n",
       "             b'This_way': 12,\n",
       "             b'Mister': 80,\n",
       "             b'way_Mister': 1,\n",
       "             b'President': 189,\n",
       "             b'Mister_President': 1,\n",
       "             b'Conductor': 20,\n",
       "             b'Conductor_Now': 1,\n",
       "             b'boarding': 13,\n",
       "             b'Now_boarding': 2,\n",
       "             b'on': 7825,\n",
       "             b'boarding_on': 1,\n",
       "             b'track': 55,\n",
       "             b'on_track': 4,\n",
       "             b'5': 22,\n",
       "             b'track_5': 1,\n",
       "             b'5_The': 1,\n",
       "             b'afternoon': 80,\n",
       "             b'The_afternoon': 1,\n",
       "             b'delight': 7,\n",
       "             b'afternoon_delight': 1,\n",
       "             b'coming': 427,\n",
       "             b'delight_coming': 1,\n",
       "             b'coming_to': 51,\n",
       "             b'Shelbyville': 105,\n",
       "             b'to_Shelbyville': 11,\n",
       "             b'Parkville': 1,\n",
       "             b'Shelbyville_Parkville': 1,\n",
       "             b'and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'Parkville_and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'Bergstrom_Hey': 1,\n",
       "             b'Hey_Mr': 18,\n",
       "             b'BERGSTROM': 57,\n",
       "             b'BERGSTROM_Hey': 3,\n",
       "             b'Hey_Lisa': 24,\n",
       "             b'indeed': 32,\n",
       "             b'Lisa_indeed': 1,\n",
       "             b'BERGSTROM_What': 1,\n",
       "             b'What_What': 37,\n",
       "             b'What_is': 211,\n",
       "             b'is_it': 313,\n",
       "             b'Oh': 7634,\n",
       "             b'Simpson_Oh': 3467,\n",
       "             b'Oh_I': 484,\n",
       "             b'mean': 1062,\n",
       "             b'I_mean': 473,\n",
       "             b'were': 2748,\n",
       "             b'mean_were': 6,\n",
       "             b'were_you': 56,\n",
       "             b'you_just': 288,\n",
       "             b'going': 1774,\n",
       "             b'just_going': 31,\n",
       "             b'going_to': 1045,\n",
       "             b'leave': 439,\n",
       "             b'to_leave': 104,\n",
       "             b'leave_just': 2,\n",
       "             b'just_like': 202,\n",
       "             b'like_that': 328,\n",
       "             b'Ah': 785,\n",
       "             b'BERGSTROM_Ah': 2,\n",
       "             b'Im': 9214,\n",
       "             b'Ah_Im': 4,\n",
       "             b'sorry': 894,\n",
       "             b'Im_sorry': 556,\n",
       "             b'sorry_Lisa': 8,\n",
       "             b'You': 6519,\n",
       "             b'Lisa_You': 28,\n",
       "             b'You_know': 897,\n",
       "             b'know_its': 96,\n",
       "             b'its_the': 136,\n",
       "             b'the_life': 36,\n",
       "             b'life_of': 30,\n",
       "             b'of_the': 2174,\n",
       "             b'substitute': 21,\n",
       "             b'the_substitute': 3,\n",
       "             b'teacher': 143,\n",
       "             b'substitute_teacher': 4,\n",
       "             b'teacher_hes': 1,\n",
       "             b'hes_a': 85,\n",
       "             b'fraud': 31,\n",
       "             b'a_fraud': 11,\n",
       "             b'Today': 96,\n",
       "             b'fraud_Today': 1,\n",
       "             b'Today_he': 1,\n",
       "             b'might': 526,\n",
       "             b'he_might': 13,\n",
       "             b'might_be': 121,\n",
       "             b'wearing': 158,\n",
       "             b'be_wearing': 7,\n",
       "             b'gym': 53,\n",
       "             b'wearing_gym': 1,\n",
       "             b'shorts': 43,\n",
       "             b'gym_shorts': 2,\n",
       "             b'tomorrow': 325,\n",
       "             b'shorts_tomorrow': 1,\n",
       "             b'tomorrow_hes': 1,\n",
       "             b'speaking': 44,\n",
       "             b'hes_speaking': 1,\n",
       "             b'French': 123,\n",
       "             b'speaking_French': 2,\n",
       "             b'or': 1564,\n",
       "             b'French_or': 1,\n",
       "             b'or_or': 4,\n",
       "             b'pretending': 21,\n",
       "             b'or_pretending': 1,\n",
       "             b'pretending_to': 13,\n",
       "             b'to_know': 238,\n",
       "             b'know_how': 271,\n",
       "             b'how_to': 217,\n",
       "             b'run': 302,\n",
       "             b'to_run': 43,\n",
       "             b'run_a': 20,\n",
       "             b'band': 102,\n",
       "             b'a_band': 10,\n",
       "             b'saw': 316,\n",
       "             b'band_saw': 1,\n",
       "             b'saw_or': 1,\n",
       "             b'God': 1235,\n",
       "             b'or_God': 2,\n",
       "             b'knows': 199,\n",
       "             b'God_knows': 6,\n",
       "             b'what': 4064,\n",
       "             b'knows_what': 19,\n",
       "             b'Simpson_You': 1811,\n",
       "             b'cant': 2413,\n",
       "             b'You_cant': 213,\n",
       "             b'go': 3162,\n",
       "             b'cant_go': 62,\n",
       "             b'Youre': 1903,\n",
       "             b'go_Youre': 5,\n",
       "             b'Youre_the': 131,\n",
       "             b'best': 643,\n",
       "             b'the_best': 324,\n",
       "             b'best_teacher': 1,\n",
       "             b'Ill': 3093,\n",
       "             b'teacher_Ill': 1,\n",
       "             b'ever': 1209,\n",
       "             b'Ill_ever': 7,\n",
       "             b'ever_have': 21,\n",
       "             b'thats': 1958,\n",
       "             b'Ah_thats': 19,\n",
       "             b'not': 5080,\n",
       "             b'thats_not': 100,\n",
       "             b'true': 356,\n",
       "             b'not_true': 31,\n",
       "             b'Other': 43,\n",
       "             b'true_Other': 1,\n",
       "             b'teachers': 79,\n",
       "             b'Other_teachers': 1,\n",
       "             b'teachers_will': 3,\n",
       "             b'come': 1491,\n",
       "             b'will_come': 25,\n",
       "             b'along': 167,\n",
       "             b'come_along': 9,\n",
       "             b'who': 1755,\n",
       "             b'along_who': 1,\n",
       "             b'please': 857,\n",
       "             b'Oh_please': 52,\n",
       "             b'BERGSTROM_No': 2,\n",
       "             b'No_I': 247,\n",
       "             b'I_cant': 1164,\n",
       "             b'lie': 169,\n",
       "             b'cant_lie': 4,\n",
       "             b'lie_to': 37,\n",
       "             b'to_you': 536,\n",
       "             b'you_I': 220,\n",
       "             b'am': 1428,\n",
       "             b'I_am': 1023,\n",
       "             b'am_the': 53,\n",
       "             b'But': 3513,\n",
       "             b'best_But': 1,\n",
       "             b'But_you': 199,\n",
       "             b'they': 2028,\n",
       "             b'know_they': 20,\n",
       "             b'need': 1528,\n",
       "             b'they_need': 11,\n",
       "             b'me': 10316,\n",
       "             b'need_me': 22,\n",
       "             b'over': 1400,\n",
       "             b'me_over': 16,\n",
       "             b'over_in': 14,\n",
       "             b'in_the': 2893,\n",
       "             b'projects': 15,\n",
       "             b'the_projects': 1,\n",
       "             b'projects_of': 2,\n",
       "             b'of_Capital': 3,\n",
       "             b'Simpson_But': 894,\n",
       "             b'But_I': 511,\n",
       "             b'I_need': 500,\n",
       "             b'need_you': 80,\n",
       "             b'Thats': 2451,\n",
       "             b'BERGSTROM_Thats': 2,\n",
       "             b'Thats_the': 200,\n",
       "             b'problem': 386,\n",
       "             b'the_problem': 53,\n",
       "             b'with': 5933,\n",
       "             b'problem_with': 38,\n",
       "             b'being': 532,\n",
       "             b'with_being': 4,\n",
       "             b'middle': 80,\n",
       "             b'being_middle': 1,\n",
       "             b'class': 264,\n",
       "             b'middle_class': 4,\n",
       "             b'Anybody': 36,\n",
       "             b'class_Anybody': 1,\n",
       "             b'Anybody_who': 2,\n",
       "             b'really': 1599,\n",
       "             b'who_really': 5,\n",
       "             b'cares': 64,\n",
       "             b'really_cares': 2,\n",
       "             b'cares_will': 1,\n",
       "             b'abandon': 11,\n",
       "             b'will_abandon': 1,\n",
       "             b'abandon_you': 3,\n",
       "             b'you_for': 220,\n",
       "             b'those': 1290,\n",
       "             b'for_those': 20,\n",
       "             b'those_who': 24,\n",
       "             b'who_need': 8,\n",
       "             b'need_it': 31,\n",
       "             b'more': 1827,\n",
       "             b'it_more': 15,\n",
       "             b'I_I': 109,\n",
       "             b'understand': 291,\n",
       "             b'I_understand': 76,\n",
       "             b'understand_Mr': 1,\n",
       "             b'Bergstrom_Im': 1,\n",
       "             b'Im_going': 348,\n",
       "             b'miss': 300,\n",
       "             b'to_miss': 18,\n",
       "             b'miss_you': 63,\n",
       "             b'BERGSTROM_Ill': 1,\n",
       "             b'tell': 1257,\n",
       "             b'Ill_tell': 116,\n",
       "             b'tell_you': 373,\n",
       "             b'you_what': 97,\n",
       "             b'Whenever': 21,\n",
       "             b'BERGSTROM_Whenever': 1,\n",
       "             b'Whenever_you': 4,\n",
       "             b'feel': 777,\n",
       "             b'you_feel': 124,\n",
       "             b'feel_like': 139,\n",
       "             b'youre': 3022,\n",
       "             b'like_youre': 56,\n",
       "             b'alone': 251,\n",
       "             b'youre_alone': 5,\n",
       "             b'alone_and': 5,\n",
       "             b'and_theres': 35,\n",
       "             b'nobody': 111,\n",
       "             b'theres_nobody': 6,\n",
       "             b'nobody_you': 1,\n",
       "             b'can': 4187,\n",
       "             b'you_can': 690,\n",
       "             b'rely': 5,\n",
       "             b'can_rely': 1,\n",
       "             b'rely_on': 5,\n",
       "             b'on_this': 208,\n",
       "             b'this_is': 1007,\n",
       "             b'is_all': 125,\n",
       "             b'all_you': 90,\n",
       "             b'you_need': 167,\n",
       "             b'need_to': 316,\n",
       "             b'Thank': 682,\n",
       "             b'Simpson_Thank': 210,\n",
       "             b'Thank_you': 596,\n",
       "             b'you_Mr': 44,\n",
       "             b'All': 1756,\n",
       "             b'Conductor_All': 3,\n",
       "             b'aboard': 31,\n",
       "             b'All_aboard': 8,\n",
       "             b'So': 2035,\n",
       "             b'Simpson_So': 716,\n",
       "             b'So_I': 151,\n",
       "             b'guess': 771,\n",
       "             b'I_guess': 689,\n",
       "             b'guess_this': 36,\n",
       "             b'It': 1568,\n",
       "             b'it_It': 24,\n",
       "             b'It_you': 1,\n",
       "             b'you_dont': 509,\n",
       "             b'mind': 362,\n",
       "             b'dont_mind': 51,\n",
       "             b'mind_Ill': 3,\n",
       "             b'Ill_just': 240,\n",
       "             b'just_run': 3,\n",
       "             b'alongside': 4,\n",
       "             b'run_alongside': 1,\n",
       "             b'alongside_the': 1,\n",
       "             b'the_train': 11,\n",
       "             b'train_as': 1,\n",
       "             b'as_it': 56,\n",
       "             b'speeds': 1,\n",
       "             b'it_speeds': 1,\n",
       "             b'speeds_you': 1,\n",
       "             b'you_from': 56,\n",
       "             b'from_my': 112,\n",
       "             b'my_life': 321,\n",
       "             b'Goodbye': 163,\n",
       "             b'BERGSTROM_Goodbye': 1,\n",
       "             b'Goodbye_Lisa': 8,\n",
       "             b'honey': 454,\n",
       "             b'Lisa_honey': 23,\n",
       "             b'Itll': 79,\n",
       "             b'honey_Itll': 2,\n",
       "             b'Itll_be': 31,\n",
       "             b'okay': 694,\n",
       "             b'be_okay': 36,\n",
       "             b'okay_Just': 6,\n",
       "             b'read': 371,\n",
       "             b'Just_read': 5,\n",
       "             b'read_the': 50,\n",
       "             b'note': 76,\n",
       "             b'the_note': 2,\n",
       "             b'Homer': 32235,\n",
       "             b'Homer_Simpson': 28284,\n",
       "             b'Never': 185,\n",
       "             b'Simpson_Never': 54,\n",
       "             b'thrown': 24,\n",
       "             b'Never_thrown': 1,\n",
       "             b'thrown_a': 3,\n",
       "             b'a_party': 51,\n",
       "             b'party_What': 1,\n",
       "             b'about_that': 175,\n",
       "             b'big': 907,\n",
       "             b'that_big': 27,\n",
       "             b'bash': 12,\n",
       "             b'big_bash': 1,\n",
       "             b'we': 5739,\n",
       "             b'bash_we': 1,\n",
       "             b'had': 1482,\n",
       "             b'we_had': 80,\n",
       "             b'had_with': 1,\n",
       "             b'with_all': 82,\n",
       "             b'champagne': 29,\n",
       "             b'the_champagne': 3,\n",
       "             b'champagne_and': 3,\n",
       "             b'musicians': 6,\n",
       "             b'and_musicians': 1,\n",
       "             b'musicians_and': 2,\n",
       "             b'holy': 33,\n",
       "             b'and_holy': 2,\n",
       "             b'men': 236,\n",
       "             b'holy_men': 1,\n",
       "             b'men_and': 15,\n",
       "             b'everything': 555,\n",
       "             b'and_everything': 20,\n",
       "             b'Simpson_Bart': 726,\n",
       "             b'Bart_didnt': 7,\n",
       "             b'get': 4467,\n",
       "             b'didnt_get': 42,\n",
       "             b'one': 4025,\n",
       "             b'get_one': 32,\n",
       "             b'one_vote': 1,\n",
       "             b'vote_Oh': 1,\n",
       "             b'Oh_this': 80,\n",
       "             b'is_the': 832,\n",
       "             b'worst': 166,\n",
       "             b'the_worst': 120,\n",
       "             b'thing': 1416,\n",
       "             b'worst_thing': 16,\n",
       "             b'thing_that': 60,\n",
       "             b'that_ever': 17,\n",
       "             b'happened': 396,\n",
       "             b'ever_happened': 17,\n",
       "             b'happened_to': 148,\n",
       "             b'us': 2323,\n",
       "             b'to_us': 83,\n",
       "             b'Alright': 43,\n",
       "             b'us_Alright': 1,\n",
       "             b'allright': 1,\n",
       "             b'Alright_allright': 1,\n",
       "             b'spilled': 10,\n",
       "             b'allright_spilled': 1,\n",
       "             b'milk': 113,\n",
       "             b'spilled_milk': 3,\n",
       "             b'milk_spilled': 2,\n",
       "             b'milk_What': 1,\n",
       "             b'are': 5527,\n",
       "             b'What_are': 447,\n",
       "             b'are_you': 1284,\n",
       "             b'you_so': 102,\n",
       "             b'mopey': 1,\n",
       "             b'so_mopey': 1,\n",
       "             b'mopey_about': 1,\n",
       "             b'Nothing': 168,\n",
       "             b'Simpson_Nothing': 72,\n",
       "             b'Marge': 15937,\n",
       "             b'Marge_Simpson': 13289,\n",
       "             b'Simpson_Lisa': 403,\n",
       "             b'Lisa_tell': 5,\n",
       "             b'tell_your': 35,\n",
       "             b'father': 557,\n",
       "             b'your_father': 205,\n",
       "             b'Bergstrom_left': 1,\n",
       "             b'today': 542,\n",
       "             b'left_today': 1,\n",
       "             b'Hes': 955,\n",
       "             b'Simpson_Hes': 192,\n",
       "             b'gone': 416,\n",
       "             b'Hes_gone': 13,\n",
       "             b'Forever': 11,\n",
       "             b'gone_Forever': 1,\n",
       "             b'And': 4992,\n",
       "             b'Simpson_And': 940,\n",
       "             b'didnt_think': 30,\n",
       "             b'youd': 341,\n",
       "             b'think_youd': 18,\n",
       "             b'youd_understand': 2,\n",
       "             b'Hey_just': 5,\n",
       "             b'because': 724,\n",
       "             b'just_because': 56,\n",
       "             b'because_I': 114,\n",
       "             b'care': 465,\n",
       "             b'dont_care': 136,\n",
       "             b'doesnt': 651,\n",
       "             b'care_doesnt': 1,\n",
       "             b'doesnt_mean': 39,\n",
       "             b'mean_I': 56,\n",
       "             b'dont_understand': 88,\n",
       "             b'Simpson_Im': 1276,\n",
       "             b'glad': 218,\n",
       "             b'Im_glad': 115,\n",
       "             b'glad_Im': 8,\n",
       "             b'Im_not': 927,\n",
       "             b'crying': 62,\n",
       "             b'not_crying': 4,\n",
       "             b'crying_because': 2,\n",
       "             b'would': 1757,\n",
       "             b'I_would': 237,\n",
       "             b'hate': 370,\n",
       "             b'would_hate': 3,\n",
       "             b'hate_for': 2,\n",
       "             b'for_you': 625,\n",
       "             b'you_to': 885,\n",
       "             b'to_think': 146,\n",
       "             b'think_that': 73,\n",
       "             b'that_what': 21,\n",
       "             b'what_Im': 93,\n",
       "             b'Im_about': 20,\n",
       "             b'about_to': 144,\n",
       "             b'say_is': 19,\n",
       "             b'based': 42,\n",
       "             b'is_based': 7,\n",
       "             b'based_on': 38,\n",
       "             b'emotion': 7,\n",
       "             b'on_emotion': 1,\n",
       "             b'emotion_But': 1,\n",
       "             b'sir': 832,\n",
       "             b'you_sir': 46,\n",
       "             b'sir_are': 7,\n",
       "             b'are_a': 141,\n",
       "             b'baboon': 9,\n",
       "             b'a_baboon': 4,\n",
       "             b'Me': 295,\n",
       "             b'Simpson_Me': 93,\n",
       "             b'Simpson_Yes': 513,\n",
       "             b'Yes_you': 52,\n",
       "             b'Baboon': 4,\n",
       "             b'you_Baboon': 1,\n",
       "             b'Baboon_baboon': 1,\n",
       "             b'baboon_baboon': 2,\n",
       "             b'realize': 109,\n",
       "             b'you_realize': 31,\n",
       "             b'realize_what': 11,\n",
       "             b'what_youre': 85,\n",
       "             b'saying': 342,\n",
       "             b'youre_saying': 38,\n",
       "             b'Simpson_Baboon': 1,\n",
       "             b'Whoa': 416,\n",
       "             b'Simpson_Whoa': 162,\n",
       "             b'somebody': 116,\n",
       "             b'Whoa_somebody': 1,\n",
       "             b'somebody_was': 1,\n",
       "             b'bound': 19,\n",
       "             b'was_bound': 1,\n",
       "             b'bound_to': 11,\n",
       "             b'say_it': 88,\n",
       "             b'it_one': 12,\n",
       "             b'day': 1269,\n",
       "             b'one_day': 69,\n",
       "             b'day_I': 57,\n",
       "             b'just_cant': 66,\n",
       "             b'believe': 723,\n",
       "             b'cant_believe': 292,\n",
       "             b'believe_it': 90,\n",
       "             b'her': 1728,\n",
       "             b'was_her': 7,\n",
       "             b'Did': 451,\n",
       "             b'Simpson_Did': 163,\n",
       "             b'Did_you': 285,\n",
       "             b'hear': 596,\n",
       "             b'you_hear': 72,\n",
       "             b'hear_that': 70,\n",
       "             b'that_Marge': 22,\n",
       "             b'She': 382,\n",
       "             b'Marge_She': 2,\n",
       "             b'called': 381,\n",
       "             b'She_called': 4,\n",
       "             b'called_me': 22,\n",
       "             b'me_a': 392,\n",
       "             b'baboon_The': 1,\n",
       "             b'stupidest': 16,\n",
       "             b'The_stupidest': 1,\n",
       "             b'ugliest': 6,\n",
       "             b'stupidest_ugliest': 1,\n",
       "             b'smelliest': 1,\n",
       "             b'ugliest_smelliest': 1,\n",
       "             b'ape': 25,\n",
       "             b'smelliest_ape': 1,\n",
       "             b'ape_of': 1,\n",
       "             b'them': 1437,\n",
       "             b'of_them': 149,\n",
       "             b'them_all': 41,\n",
       "             b'Simpson_Homer': 897,\n",
       "             b'Homer_you': 128,\n",
       "             b'you_are': 493,\n",
       "             b'are_not': 79,\n",
       "             b'allowed': 76,\n",
       "             b'not_allowed': 38,\n",
       "             b'allowed_to': 43,\n",
       "             b'to_have': 320,\n",
       "             b'hurt': 280,\n",
       "             b'have_hurt': 2,\n",
       "             b'feelings': 109,\n",
       "             b'hurt_feelings': 3,\n",
       "             b'feelings_right': 1,\n",
       "             b'right_now': 245,\n",
       "             b'Theres': 711,\n",
       "             b'now_Theres': 5,\n",
       "             b'Theres_a': 142,\n",
       "             b'girl': 527,\n",
       "             b'little_girl': 120,\n",
       "             b'upstairs': 30,\n",
       "             b'girl_upstairs': 1,\n",
       "             b'upstairs_who': 1,\n",
       "             b'needs': 263,\n",
       "             b'who_needs': 25,\n",
       "             b'needs_you': 12,\n",
       "             b'Her': 57,\n",
       "             b'you_Her': 1,\n",
       "             b'confidence': 19,\n",
       "             b'Her_confidence': 1,\n",
       "             b'confidence_in': 1,\n",
       "             b'in_her': 38,\n",
       "             b'her_father': 11,\n",
       "             b'father_is': 28,\n",
       "             b'shaken': 5,\n",
       "             b'is_shaken': 1,\n",
       "             b'shaken_and': 1,\n",
       "             b'no': 3459,\n",
       "             b'and_no': 58,\n",
       "             ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(word_tokens, min_count=100, progress_per=5000)\n",
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-07 23:25:59,587 : INFO : source_vocab length 526421\n",
      "2020-06-07 23:26:10,259 : INFO : Phraser built with 134 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Landlady', 'I', 'think', 'hes', 'taking', 'the', 'next', 'train', 'to', 'Capital', 'City']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agregamos las celdas para entrenar al modelo, para hacer esto utilizamos que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons_word2vec = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=4,\n",
    "    size=200,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-08 01:20:13,943 : INFO : collecting all words and their counts\n",
      "2020-06-08 01:20:13,947 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-08 01:20:14,465 : INFO : PROGRESS: at sentence #10000, processed 111639 words, keeping 12494 word types\n",
      "2020-06-08 01:20:14,958 : INFO : PROGRESS: at sentence #20000, processed 225335 words, keeping 19455 word types\n",
      "2020-06-08 01:20:15,504 : INFO : PROGRESS: at sentence #30000, processed 348717 words, keeping 25712 word types\n",
      "2020-06-08 01:20:15,994 : INFO : PROGRESS: at sentence #40000, processed 463626 words, keeping 30091 word types\n",
      "2020-06-08 01:20:16,464 : INFO : PROGRESS: at sentence #50000, processed 570988 words, keeping 34183 word types\n",
      "2020-06-08 01:20:16,907 : INFO : PROGRESS: at sentence #60000, processed 672231 words, keeping 37740 word types\n"
     ]
    }
   ],
   "source": [
    "# construyo el vocabulario\n",
    "simpsons_word2vec.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oos6kUF5tYJr"
   },
   "source": [
    "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. Cúal es la diferencia entre ambos resultados? Por qué ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escogería uno vs el otro? (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wqX03jStYJr"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3ovjW6WtYJr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAbldwNFtYJu"
   },
   "source": [
    "### Parte 4: Aplicar embeddings para clasificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5DlKSestYJu"
   },
   "source": [
    "Ahora utilizaremos estos embeddings para clasificar palabras basadas en su polaridad (positivas o negativas). Para esto ocuparemos el lexicón AFINN incluido en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCDFYm_ytYJu"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-icASpvytYJw"
   },
   "source": [
    "Hint: Para w2v son esperables KeyErrors, para eso pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar), para luego aplicar los embeddings en toda la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXw9CGmgtYJx"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8d8qtIgtYJy"
   },
   "source": [
    "**Pregunta 1**: Una vez que tengan un dataframe del estilo [embedding, sentimiento] para ambos modelos, separarlo utilizando la siguiente función, donde X es su columna de embeddings e y es la columna de los valores. (3 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_frb5aDatYJz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Dw6KBAftYJ1"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmrrkpaStYJ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u33__LaNtYJ3"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dP5zYH3qtYJ3"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOt6N4nftYJ4"
   },
   "source": [
    "# Bonus: 2 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnsV0J6StYJ4"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim (1 punto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lYudsa_tYJ4"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd4psA7LtYJ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrZxZZfutYJ7"
   },
   "source": [
    "**Pregunta 2**: Utilizar wefe para ver si el modelo w2v entrenado con los dialogos de los Simpson tienen algun bias entre los personajes hombres y la cerveza (1 punto):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Subk47EatYJ7"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgThotyEtYJ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minitarea3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
