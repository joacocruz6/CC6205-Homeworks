{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "colab_type": "text",
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "# Minitarea 3\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "Nombre: Joaquin Cruz\n",
    "\n",
    "Fecha de Entrega: Domingo 17 de Mayo\n",
    "\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "- El ejercicio consiste en:\n",
    "\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    \n",
    "    - Entrenar Word2Vec y FastText sobre un pequeño corpus.\n",
    "    \n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "\n",
    "- La minitarea es INDIVIDUAL.\n",
    "\n",
    "- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n",
    "\n",
    "- La entrega debe ser por u-cursos.\n",
    "\n",
    "- Atrasos: se descontará un punto por día hábil de atraso tanto para las mini-tareas como para las competencias.\n",
    "\n",
    "- En el horario de auxiliar se abrirán horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n",
    "\n",
    "- Cada sección tiene un punto base y se evalúa sobre 6 puntos.\n",
    "\n",
    "- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores.\n",
    "\n",
    "\n",
    "## Referencias   \n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## Preguntas Teóricas\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### Parte 1: Modelos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. (3 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Representación escogida del documento de entrada: \n",
    "\n",
    "Como estamos viendo pertenencia a clases, puede ser util un modelo BOW simple, pero puede ser muy limitado y el modelo se puede portar mal sobre palabras que esten en la 'intersección' de las clases (por ejemplo si un documento dice ministra del deporte, y esta etiquetado como politica pues fue un cambio de gabinete el modelo puede llegar a portarse mal con respecto a ese tipo de articulos). Por tanto un añadir una feature al modelo sobre los trigramas puede portarse mucho mejor pues puede capturar un poco más el contexto de las palabras y asi poder descriminar mejor entre los documentos (el de bigramas tambien podria servir, pero eso seria mas util cuando hay analisis de sentimientos involucrado y aqui no nos afectan tanto las negaciones, o pares de palabras pueden tambien producir el efecto anterior descrito). Asi la representación escogida seria BOW con features de trigramas.\n",
    "\n",
    "\n",
    "Parámetros del modelo:\n",
    "\n",
    "Los parametros del modelo a definir son los pesos de la matriz $W$ que son representados en el modelo lineal $\\vec{\\hat{y}} = \\vec{x}\\cdot W + \\vec{b}, en este caso estos representan el peso que le atribuye el modelo a las palabras y los trigramas en de los articulos con cada clase.\n",
    "\n",
    "\n",
    "Transformaciones necesarias:\n",
    "\n",
    "Hay que aplicar la funcion sigmoide al output de cada predicción, pues esta funcion mapea los resultados del modelo lineal a el rango $[0,1]$ y nos entrega una distribucion de probabilidad en el vector de salida del modelo que seria ''el grado de pertenencia a cada clase''\n",
    "\n",
    "\n",
    "Función de pérdida escogida:\n",
    "\n",
    "Como se pasa por una transformacion de softmax, y el problema es multiclase entonces seria mejor escoger una Categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique el proceso de entrenamiento y evaluación del modelo. (3 puntos)\n",
    "\n",
    "**Respuesta**: El modelo entrena bajo el algoritmo de descenso estocastico. En este se inician todos los parametros de la matriz $W$ del bias  $b$ de forma aleatoria. Luego, consideremos $\\Theta$ como estos parametros, entonces se calcula el gradiente de la funcion de perdida $\\mathcal{L}$ sobre tales parametros, y se van acumuland en cada paso que se hace hasta converger (es decir que no se avance tanto del minimo o se cumple cierta cantidad de ciclos). Ahora esto es la dirección de maximo crecimiento de tal funcion, por tanto se deben modificar los parametros $\\Theta$ en la direccion contraria al gradiente acumulado, i.e se genera $\\Theta = \\Theta - \\hat{g}$, donde $\\hat{g}$ es la suma acumulada de los gradientes de la loss hasta ese ciclo o epoca de entrenamiento. Podemos regular cuanto podemos avanzar en cada paso, con un hiperparametro $\\mu$, para ir actualizando $\\Theta$, en ese caso cada vez que descendemos del gradiente de la los, los parametros se calcularian como: $\\Theta = \\Theta - \\mu\\cdot\\hat{g}$. Asi se repite en todos los inputs del set de entrenamiento por epocas o hasta que los cambios de los parametros no sean significativos (que seria que los parametros estan convergiendo). Al final tendremos en $\\Theta$ unu minimo de loss local, optimos para los valores iniciales de la loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### Parte 2: Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![Red](https://drive.google.com/uc?id=1Yd0s9g5SlB1-XuVokGQO2J-yDudQe2Kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, represéntela matemáticamente, entregando las dimensiones de las matrices y vectores. (3 Puntos)\n",
    "\n",
    "**Respuesta**: Considerando un bias $b^{i}$ para la $i$-esima capa, con $i \\in {1,2,3,4}$, tenemos que la descripcion matematica en cada capa es la evaluacion del modelo lineal pasado por la funcion de activación. Por tanto desde que el input \"entra\" en la red hasta que sale son aplicadas distintas transformaciones lineales en cada capa como sigue:\n",
    "\n",
    "Utilizando la notacion de que $g(\\vec{x}) = (g(x_{1}),g(x_{2}),...,g(x_{n}))$ con $n = dim(\\vec{x})$ al utilizar las funciones de activacion que son $g,f,h: \\mathbb{R} \\to \\mathbb{R}$\n",
    "\n",
    "$\\vec{h^{1}} = \\vec{x} \\cdot W^{1} + \\vec{b^{1}}$ \n",
    "\n",
    "con $dim(\\vec{x}) = 3;\\;dim(W^{1})=3\\times2;\\;dim(\\vec{b^{1}})=2$\n",
    "\n",
    "$\\vec{z^{1}} = g(\\vec{h^{1}})$\n",
    "\n",
    "con $dim(g(\\vec{h^{1}})) = 2\\;dim(\\vec{z^{1}}) = 2$\n",
    "\n",
    "$\\vec{h^{2}} = \\vec{z^{1}} \\cdot W^{2} + \\vec{b^{2}}$\n",
    "\n",
    "con $dim(\\vec{z^{1}}) = 2;\\;dim(W^{2})=2\\times3;\\;dim(\\vec{b^{2}})=3$\n",
    "\n",
    "$\\vec{z^{2}} = f(\\vec{h^{2}})$\n",
    "\n",
    "con $dim(f(\\vec{h^{2}})) = 3\\;dim(\\vec{z^{2}}) = 3$\n",
    "\n",
    "$\\vec{h^{3}} = \\vec{z^{2}} \\cdot W^{3} + \\vec{b^{3}}$\n",
    "\n",
    "con $dim(\\vec{z^{2}}) = 3;\\;dim(W^{3})=3\\times1;\\;dim(\\vec{b^{3}})=1$\n",
    "\n",
    "$\\vec{z^{3}} = h(\\vec{h^{3}})$\n",
    "\n",
    "con $dim(h(\\vec{h^{3}})) = 1\\;dim(\\vec{z^{3}}) = 1$\n",
    "\n",
    "$\\vec{\\hat{y}} = \\vec{z^{3}} \\cdot W^{4} + \\vec{b^{4}}$\n",
    "\n",
    "con $dim(\\vec{z^{3}}) = 1;\\;dim(W^{4})=1\\times4;\\;dim(\\vec{b^{4}})=4$\n",
    "\n",
    "\n",
    "Por lo tanto, reemplazando nos da que la formula es:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(f(g(\\vec{x} \\cdot W^{1} + \\vec{b^{1}})\\cdot W^{2} + \\vec{b^{2}})\\cdot W^{3} + \\vec{b^{3}})\\cdot W^{4} + \\vec{b^{4}}$\n",
    "\n",
    "Con $dim(\\vec{\\hat{y}}) = 4$\n",
    "\n",
    "**Pregunta 2**: Qué es backpropagation? Cuales serían los parámetros a evaluar en la red neuronal anterior? (1 punto)\n",
    "\n",
    "**Respuesta**: Backpropagation es un algoritmo que nos facilita el calculo del gradiente de la función de perdida cuando debemos entrenar a las redes neuronales. El objetivo del algoritmo es el calculo del gradiente de la función de perdida aplicada en la red con respecto a los parametros que esta debe estimar, que son los pesos de cada paso, es decir los $W^{i}, \\vec{b^{i}}$ utilizando de forma inteligente la regla de la cadena de las derivadas.\n",
    "\n",
    "Como se dijo anteriormente, los parametros a evaluar en la red anterior son: $W^{1},W^{2},W^{3},W^{4}$ y $\\vec{b^{1}},\\vec{b^{2}},\\vec{b^{3}},\\vec{b^{4}}$\n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? (2 puntos)\n",
    "\n",
    "**Respuesta**: Por simplicidad, tomemos que podemos incluir los bias en los $W_{i}$. Por tanto, el procedimiento funciona como:\n",
    "\n",
    "1. Se aplica la entrada $\\vec{x}$ a la red y se hace forward pass sacando los $\\vec{h^{l}_{j}}$ y los $\\vec{z^{l}_{j}}$ donde $l$ es la capa en que esto es calculado (en este caso $l \\in \\{1,2,3,4\\}$ e j es el elemento del vector que correspondiente en cada capa.\n",
    "\n",
    "2. Se evaluan los $\\vec{\\delta^{m}_{j}$ en cada output de los forward pass correspondiente, en los calculos que hacemos en los forward pass directamente, es decir, entre el output del computo anterio y el computo actual, pues esto son faciles de calcular.\n",
    "\n",
    "3. Luego se hace backpropagation sobre las capas $\\vec{\\delta^{l+1}_{k}}$, con esto podemos obtener de manera eficiente el $\\vec{\\delta^{l}_{k}}$ en cada capa. Este proceso se hace desde la capa mas externa (la que tiene el output) hacia la mas interna (la que tiene el input).\n",
    "\n",
    "4. Finalmente calculamos las derivada de la loss usando la ecuación:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{l}_{[i,j]}} = \\vec{\\delta^{l}_{j}}\\times \\vec{z^{l-1}_{i}}  $$\n",
    "\n",
    "\n",
    "Ahora para obtener los valores de cada $\\vec{\\delta^l_{[j]}}$, esta dado por:\n",
    "\n",
    "$$\\vec{\\delta^l_{[j]}} = g'(\\vec{h^{l}_{j}}) \\times \\sum_{k}(\\vec{\\delta^{l+1}_{[j]}} \\times W^{l+1}_{j,k})$$\n",
    "\n",
    "indicandonos que  tenemos que sacar los $\\delta$ de las capas hacia mas afuera (cercanas al output de la red) y siendo propagadas hacia la capa del $\\delta$ a obtener\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## Pregunta Práctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### Parte 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecCvnryeQiG7"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from typing import Tuple\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "model_size = 200\n",
    "models_min_count = 10\n",
    "train_epochs = 15\n",
    "vocab_progress_per = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY3kmg4onnsu"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias\n",
    "dataset = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. (4 puntos) (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**: Veamos un poco los datos que tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YILUICGtYJo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       raw_character_text                                       spoken_words\n",
      "52585        Lisa Simpson               Ooh, it's morning. I gotta get back!\n",
      "108416        Moe Szyslak  Easy, easy there, Lenny. You can always play i...\n",
      "76833       Homer Simpson  Wake up! The Rapture is coming in half an hour...\n",
      "72223       Marge Simpson             Bart! That hobo skeleton is not a toy!\n",
      "56126               Muntu                     I don't want to talk about it.\n",
      "33890         Moe Szyslak  Wha-- Uh-oh. Here comes the evening rush. Clea...\n",
      "79300      Grampa Simpson     Well, that's the craziest thing I'll ever see.\n",
      "92085        Lisa Simpson                      Bald Man's Basketball League?\n",
      "129323        Lionel Hutz            Now, we'll get a real doctor's opinion!\n",
      "111548      Homer Simpson  Oh my God! This unsourced, undated video has c...\n",
      "(131853, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.sample(10))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos unir los personajes con las palabras que ellos hablan, de esta forma podemos entrenar nuestros embeddings relacionando la frase que dice cada personaje con lo que este habla, debemos poner ojo pues puntuaciones de `string.punctuation` incluyen '-' o \"'\" que son usadas en expresiones recurrentes en los simpsons (por ejemplo \"D'oh\" de Homero Simpson). Pero esto se puede remediar al agregar pares de palabras mas adelante que tengan un mejor significado (asi como lo se hizo con `nueva york` en el auxiliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset[\"raw_character_text\"] + \" \" + dataset[\"spoken_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "word_tokens = [simple_tokenizer(doc) for doc in content.values]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lisa', 'Simpson', 'Wheres', 'Mr', 'Bergstrom']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora juntamos frases que puedan ser vistas como una a partir de como se distribuyen en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 10:47:15,223 : INFO : collecting all words and their counts\n",
      "2020-06-09 10:47:15,225 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-06-09 10:47:15,385 : INFO : PROGRESS: at sentence #5000, processed 58962 words and 38714 word types\n",
      "2020-06-09 10:47:15,517 : INFO : PROGRESS: at sentence #10000, processed 116303 words and 67474 word types\n",
      "2020-06-09 10:47:15,649 : INFO : PROGRESS: at sentence #15000, processed 171261 words and 92181 word types\n",
      "2020-06-09 10:47:15,793 : INFO : PROGRESS: at sentence #20000, processed 234725 words and 118440 word types\n",
      "2020-06-09 10:47:15,948 : INFO : PROGRESS: at sentence #25000, processed 297388 words and 143880 word types\n",
      "2020-06-09 10:47:16,119 : INFO : PROGRESS: at sentence #30000, processed 363426 words and 169356 word types\n",
      "2020-06-09 10:47:16,280 : INFO : PROGRESS: at sentence #35000, processed 426138 words and 191490 word types\n",
      "2020-06-09 10:47:16,463 : INFO : PROGRESS: at sentence #40000, processed 483260 words and 210723 word types\n",
      "2020-06-09 10:47:16,603 : INFO : PROGRESS: at sentence #45000, processed 539593 words and 229784 word types\n",
      "2020-06-09 10:47:16,739 : INFO : PROGRESS: at sentence #50000, processed 594804 words and 248360 word types\n",
      "2020-06-09 10:47:16,894 : INFO : PROGRESS: at sentence #55000, processed 648793 words and 265884 word types\n",
      "2020-06-09 10:47:17,019 : INFO : PROGRESS: at sentence #60000, processed 700367 words and 281851 word types\n",
      "2020-06-09 10:47:17,178 : INFO : PROGRESS: at sentence #65000, processed 755232 words and 298821 word types\n",
      "2020-06-09 10:47:17,328 : INFO : PROGRESS: at sentence #70000, processed 816341 words and 318657 word types\n",
      "2020-06-09 10:47:17,493 : INFO : PROGRESS: at sentence #75000, processed 877055 words and 337366 word types\n",
      "2020-06-09 10:47:17,881 : INFO : PROGRESS: at sentence #80000, processed 938145 words and 355645 word types\n",
      "2020-06-09 10:47:18,181 : INFO : PROGRESS: at sentence #85000, processed 998833 words and 373741 word types\n",
      "2020-06-09 10:47:18,342 : INFO : PROGRESS: at sentence #90000, processed 1057552 words and 391071 word types\n",
      "2020-06-09 10:47:18,490 : INFO : PROGRESS: at sentence #95000, processed 1118175 words and 408042 word types\n",
      "2020-06-09 10:47:18,630 : INFO : PROGRESS: at sentence #100000, processed 1177810 words and 425339 word types\n",
      "2020-06-09 10:47:18,900 : INFO : PROGRESS: at sentence #105000, processed 1238652 words and 442481 word types\n",
      "2020-06-09 10:47:19,086 : INFO : PROGRESS: at sentence #110000, processed 1300175 words and 460051 word types\n",
      "2020-06-09 10:47:19,328 : INFO : PROGRESS: at sentence #115000, processed 1359523 words and 476101 word types\n",
      "2020-06-09 10:47:19,482 : INFO : PROGRESS: at sentence #120000, processed 1419933 words and 492590 word types\n",
      "2020-06-09 10:47:19,697 : INFO : PROGRESS: at sentence #125000, processed 1479504 words and 507885 word types\n",
      "2020-06-09 10:47:19,836 : INFO : PROGRESS: at sentence #130000, processed 1538844 words and 521438 word types\n",
      "2020-06-09 10:47:19,889 : INFO : collected 526421 word types from a corpus of 1560756 words (unigram + bigrams) and 131853 sentences\n",
      "2020-06-09 10:47:19,891 : INFO : using 526421 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'Miss': 374,\n",
       "             b'Hoover': 206,\n",
       "             b'Miss_Hoover': 198,\n",
       "             b'No': 3855,\n",
       "             b'Hoover_No': 5,\n",
       "             b'actually': 267,\n",
       "             b'No_actually': 6,\n",
       "             b'it': 12164,\n",
       "             b'actually_it': 4,\n",
       "             b'was': 5334,\n",
       "             b'it_was': 500,\n",
       "             b'a': 28723,\n",
       "             b'was_a': 607,\n",
       "             b'little': 2284,\n",
       "             b'a_little': 1107,\n",
       "             b'of': 15630,\n",
       "             b'little_of': 12,\n",
       "             b'both': 280,\n",
       "             b'of_both': 5,\n",
       "             b'Sometimes': 68,\n",
       "             b'both_Sometimes': 1,\n",
       "             b'when': 1746,\n",
       "             b'Sometimes_when': 3,\n",
       "             b'when_a': 36,\n",
       "             b'disease': 33,\n",
       "             b'a_disease': 2,\n",
       "             b'is': 11409,\n",
       "             b'disease_is': 2,\n",
       "             b'in': 11719,\n",
       "             b'is_in': 147,\n",
       "             b'all': 4882,\n",
       "             b'in_all': 31,\n",
       "             b'the': 38582,\n",
       "             b'all_the': 740,\n",
       "             b'magazines': 25,\n",
       "             b'the_magazines': 2,\n",
       "             b'and': 13527,\n",
       "             b'magazines_and': 1,\n",
       "             b'and_all': 76,\n",
       "             b'news': 259,\n",
       "             b'the_news': 29,\n",
       "             b'shows': 129,\n",
       "             b'news_shows': 1,\n",
       "             b'its': 3334,\n",
       "             b'shows_its': 3,\n",
       "             b'only': 1498,\n",
       "             b'its_only': 26,\n",
       "             b'natural': 68,\n",
       "             b'only_natural': 4,\n",
       "             b'that': 11055,\n",
       "             b'natural_that': 1,\n",
       "             b'you': 30228,\n",
       "             b'that_you': 239,\n",
       "             b'think': 2700,\n",
       "             b'you_think': 486,\n",
       "             b'think_you': 207,\n",
       "             b'have': 7191,\n",
       "             b'you_have': 738,\n",
       "             b'have_it': 84,\n",
       "             b'Lisa': 12891,\n",
       "             b'Simpson': 68843,\n",
       "             b'Lisa_Simpson': 10919,\n",
       "             b'Wheres': 254,\n",
       "             b'Simpson_Wheres': 78,\n",
       "             b'Mr': 1759,\n",
       "             b'Wheres_Mr': 2,\n",
       "             b'Bergstrom': 19,\n",
       "             b'Mr_Bergstrom': 19,\n",
       "             b'I': 35713,\n",
       "             b'Hoover_I': 11,\n",
       "             b'dont': 5235,\n",
       "             b'I_dont': 2277,\n",
       "             b'know': 4633,\n",
       "             b'dont_know': 772,\n",
       "             b'Although': 74,\n",
       "             b'know_Although': 1,\n",
       "             b'Id': 1086,\n",
       "             b'Although_Id': 1,\n",
       "             b'sure': 1127,\n",
       "             b'Id_sure': 6,\n",
       "             b'like': 5536,\n",
       "             b'sure_like': 5,\n",
       "             b'to': 26349,\n",
       "             b'like_to': 672,\n",
       "             b'talk': 593,\n",
       "             b'to_talk': 159,\n",
       "             b'talk_to': 150,\n",
       "             b'him': 2342,\n",
       "             b'to_him': 99,\n",
       "             b'He': 1057,\n",
       "             b'him_He': 10,\n",
       "             b'didnt': 1331,\n",
       "             b'He_didnt': 22,\n",
       "             b'touch': 129,\n",
       "             b'didnt_touch': 5,\n",
       "             b'my': 10680,\n",
       "             b'touch_my': 8,\n",
       "             b'lesson': 126,\n",
       "             b'my_lesson': 11,\n",
       "             b'plan': 203,\n",
       "             b'lesson_plan': 2,\n",
       "             b'What': 3551,\n",
       "             b'plan_What': 1,\n",
       "             b'did': 1922,\n",
       "             b'What_did': 88,\n",
       "             b'he': 2443,\n",
       "             b'did_he': 44,\n",
       "             b'teach': 185,\n",
       "             b'he_teach': 1,\n",
       "             b'teach_you': 59,\n",
       "             b'That': 1459,\n",
       "             b'Simpson_That': 467,\n",
       "             b'life': 1081,\n",
       "             b'That_life': 1,\n",
       "             b'life_is': 51,\n",
       "             b'worth': 140,\n",
       "             b'is_worth': 15,\n",
       "             b'living': 176,\n",
       "             b'worth_living': 5,\n",
       "             b'Edna': 861,\n",
       "             b'KrabappelFlanders': 719,\n",
       "             b'Edna_KrabappelFlanders': 719,\n",
       "             b'The': 4193,\n",
       "             b'KrabappelFlanders_The': 6,\n",
       "             b'polls': 13,\n",
       "             b'The_polls': 1,\n",
       "             b'will': 2550,\n",
       "             b'polls_will': 1,\n",
       "             b'be': 6853,\n",
       "             b'will_be': 515,\n",
       "             b'open': 299,\n",
       "             b'be_open': 3,\n",
       "             b'from': 2794,\n",
       "             b'open_from': 1,\n",
       "             b'now': 2859,\n",
       "             b'from_now': 62,\n",
       "             b'until': 280,\n",
       "             b'now_until': 2,\n",
       "             b'until_the': 30,\n",
       "             b'end': 339,\n",
       "             b'the_end': 149,\n",
       "             b'end_of': 108,\n",
       "             b'recess': 12,\n",
       "             b'of_recess': 1,\n",
       "             b'Now': 3036,\n",
       "             b'recess_Now': 1,\n",
       "             b'just': 5500,\n",
       "             b'Now_just': 25,\n",
       "             b'just_in': 41,\n",
       "             b'case': 193,\n",
       "             b'in_case': 33,\n",
       "             b'any': 1076,\n",
       "             b'case_any': 1,\n",
       "             b'any_of': 97,\n",
       "             b'of_you': 502,\n",
       "             b'decided': 84,\n",
       "             b'have_decided': 9,\n",
       "             b'decided_to': 54,\n",
       "             b'put': 870,\n",
       "             b'to_put': 124,\n",
       "             b'put_any': 2,\n",
       "             b'thought': 932,\n",
       "             b'any_thought': 1,\n",
       "             b'into': 1160,\n",
       "             b'thought_into': 4,\n",
       "             b'this': 9064,\n",
       "             b'into_this': 49,\n",
       "             b'well': 1492,\n",
       "             b'this_well': 6,\n",
       "             b'well_have': 54,\n",
       "             b'our': 2970,\n",
       "             b'have_our': 16,\n",
       "             b'final': 94,\n",
       "             b'our_final': 12,\n",
       "             b'statements': 7,\n",
       "             b'final_statements': 1,\n",
       "             b'Martin': 519,\n",
       "             b'statements_Martin': 1,\n",
       "             b'Prince': 436,\n",
       "             b'Martin_Prince': 409,\n",
       "             b'Prince_I': 26,\n",
       "             b'dont_think': 247,\n",
       "             b'theres': 1021,\n",
       "             b'think_theres': 26,\n",
       "             b'anything': 764,\n",
       "             b'theres_anything': 10,\n",
       "             b'left': 530,\n",
       "             b'anything_left': 2,\n",
       "             b'left_to': 24,\n",
       "             b'say': 1944,\n",
       "             b'to_say': 358,\n",
       "             b'Bart': 16499,\n",
       "             b'KrabappelFlanders_Bart': 40,\n",
       "             b'Bart_Simpson': 13243,\n",
       "             b'Victory': 7,\n",
       "             b'Simpson_Victory': 4,\n",
       "             b'party': 349,\n",
       "             b'Victory_party': 1,\n",
       "             b'under': 322,\n",
       "             b'party_under': 1,\n",
       "             b'under_the': 82,\n",
       "             b'slide': 31,\n",
       "             b'the_slide': 5,\n",
       "             b'Simpson_Mr': 155,\n",
       "             b'Bergstrom_Mr': 1,\n",
       "             b'Landlady': 3,\n",
       "             b'Hey': 3901,\n",
       "             b'Landlady_Hey': 1,\n",
       "             b'hey': 349,\n",
       "             b'Hey_hey': 175,\n",
       "             b'hey_he': 1,\n",
       "             b'Moved': 2,\n",
       "             b'he_Moved': 1,\n",
       "             b'out': 4258,\n",
       "             b'Moved_out': 1,\n",
       "             b'out_this': 32,\n",
       "             b'morning': 292,\n",
       "             b'this_morning': 42,\n",
       "             b'morning_He': 1,\n",
       "             b'must': 774,\n",
       "             b'He_must': 18,\n",
       "             b'must_have': 109,\n",
       "             b'have_a': 1216,\n",
       "             b'new': 1172,\n",
       "             b'a_new': 344,\n",
       "             b'job': 568,\n",
       "             b'new_job': 16,\n",
       "             b'job_he': 2,\n",
       "             b'took': 324,\n",
       "             b'he_took': 8,\n",
       "             b'his': 2019,\n",
       "             b'took_his': 5,\n",
       "             b'Copernicus': 5,\n",
       "             b'his_Copernicus': 1,\n",
       "             b'costume': 50,\n",
       "             b'Copernicus_costume': 1,\n",
       "             b'Do': 784,\n",
       "             b'Simpson_Do': 202,\n",
       "             b'Do_you': 467,\n",
       "             b'you_know': 857,\n",
       "             b'where': 1114,\n",
       "             b'know_where': 86,\n",
       "             b'where_I': 111,\n",
       "             b'could': 2147,\n",
       "             b'I_could': 543,\n",
       "             b'find': 815,\n",
       "             b'could_find': 19,\n",
       "             b'find_him': 29,\n",
       "             b'Landlady_I': 1,\n",
       "             b'I_think': 1138,\n",
       "             b'hes': 1004,\n",
       "             b'think_hes': 46,\n",
       "             b'taking': 309,\n",
       "             b'hes_taking': 3,\n",
       "             b'taking_the': 25,\n",
       "             b'next': 644,\n",
       "             b'the_next': 186,\n",
       "             b'train': 88,\n",
       "             b'next_train': 2,\n",
       "             b'train_to': 7,\n",
       "             b'Capital': 49,\n",
       "             b'to_Capital': 7,\n",
       "             b'City': 145,\n",
       "             b'Capital_City': 40,\n",
       "             b'Simpson_The': 709,\n",
       "             b'The_train': 4,\n",
       "             b'how': 1926,\n",
       "             b'train_how': 1,\n",
       "             b'how_like': 2,\n",
       "             b'like_him': 31,\n",
       "             b'traditional': 18,\n",
       "             b'him_traditional': 1,\n",
       "             b'yet': 334,\n",
       "             b'traditional_yet': 1,\n",
       "             b'environmentally': 2,\n",
       "             b'yet_environmentally': 1,\n",
       "             b'sound': 219,\n",
       "             b'environmentally_sound': 1,\n",
       "             b'Yes': 1512,\n",
       "             b'Landlady_Yes': 1,\n",
       "             b'Yes_and': 17,\n",
       "             b'and_its': 109,\n",
       "             b'been': 1846,\n",
       "             b'its_been': 40,\n",
       "             b'been_the': 32,\n",
       "             b'backbone': 7,\n",
       "             b'the_backbone': 4,\n",
       "             b'backbone_of': 4,\n",
       "             b'of_our': 266,\n",
       "             b'country': 212,\n",
       "             b'our_country': 10,\n",
       "             b'since': 350,\n",
       "             b'country_since': 1,\n",
       "             b'Leland': 1,\n",
       "             b'since_Leland': 1,\n",
       "             b'Stanford': 6,\n",
       "             b'Leland_Stanford': 1,\n",
       "             b'drove': 44,\n",
       "             b'Stanford_drove': 1,\n",
       "             b'drove_that': 1,\n",
       "             b'golden': 30,\n",
       "             b'that_golden': 5,\n",
       "             b'spike': 4,\n",
       "             b'golden_spike': 2,\n",
       "             b'at': 4221,\n",
       "             b'spike_at': 1,\n",
       "             b'Promontory': 1,\n",
       "             b'at_Promontory': 1,\n",
       "             b'point': 290,\n",
       "             b'Promontory_point': 1,\n",
       "             b'Simpson_I': 4474,\n",
       "             b'see': 2586,\n",
       "             b'I_see': 323,\n",
       "             b'see_he': 5,\n",
       "             b'touched': 38,\n",
       "             b'he_touched': 1,\n",
       "             b'touched_you': 2,\n",
       "             b'too': 1514,\n",
       "             b'you_too': 80,\n",
       "             b'Simpson_Hey': 1641,\n",
       "             b'thanks': 278,\n",
       "             b'Hey_thanks': 17,\n",
       "             b'for': 9533,\n",
       "             b'thanks_for': 59,\n",
       "             b'your': 9407,\n",
       "             b'for_your': 277,\n",
       "             b'vote': 105,\n",
       "             b'your_vote': 4,\n",
       "             b'man': 2116,\n",
       "             b'vote_man': 1,\n",
       "             b'Nelson': 1406,\n",
       "             b'Muntz': 1210,\n",
       "             b'Nelson_Muntz': 1157,\n",
       "             b'Muntz_I': 86,\n",
       "             b'I_didnt': 550,\n",
       "             b'didnt_vote': 4,\n",
       "             b'Votings': 1,\n",
       "             b'vote_Votings': 1,\n",
       "             b'Votings_for': 1,\n",
       "             b'geeks': 8,\n",
       "             b'for_geeks': 1,\n",
       "             b'Well': 5461,\n",
       "             b'Simpson_Well': 2153,\n",
       "             b'Well_you': 183,\n",
       "             b'got': 3493,\n",
       "             b'you_got': 224,\n",
       "             b'got_that': 52,\n",
       "             b'right': 3719,\n",
       "             b'that_right': 42,\n",
       "             b'Thanks': 526,\n",
       "             b'right_Thanks': 3,\n",
       "             b'Thanks_for': 152,\n",
       "             b'girls': 245,\n",
       "             b'vote_girls': 1,\n",
       "             b'Terrisherri': 3,\n",
       "             b'We': 2270,\n",
       "             b'Terrisherri_We': 2,\n",
       "             b'forgot': 140,\n",
       "             b'We_forgot': 4,\n",
       "             b'Well_dont': 31,\n",
       "             b'sweat': 25,\n",
       "             b'dont_sweat': 2,\n",
       "             b'sweat_it': 3,\n",
       "             b'Just': 1090,\n",
       "             b'it_Just': 15,\n",
       "             b'so': 3688,\n",
       "             b'Just_so': 9,\n",
       "             b'long': 651,\n",
       "             b'so_long': 45,\n",
       "             b'as': 2523,\n",
       "             b'long_as': 124,\n",
       "             b'as_a': 400,\n",
       "             b'couple': 178,\n",
       "             b'a_couple': 114,\n",
       "             b'couple_of': 83,\n",
       "             b'people': 1329,\n",
       "             b'of_people': 75,\n",
       "             b'people_did': 2,\n",
       "             b'did_right': 1,\n",
       "             b'Milhouse': 2398,\n",
       "             b'right_Milhouse': 4,\n",
       "             b'Van': 2167,\n",
       "             b'Milhouse_Van': 1757,\n",
       "             b'Houten': 2120,\n",
       "             b'Van_Houten': 2120,\n",
       "             b'Uh': 1472,\n",
       "             b'Houten_Uh': 45,\n",
       "             b'oh': 360,\n",
       "             b'Uh_oh': 49,\n",
       "             b'Lewis': 45,\n",
       "             b'Simpson_Lewis': 1,\n",
       "             b'Somebody': 70,\n",
       "             b'Simpson_Somebody': 17,\n",
       "             b'Somebody_must': 2,\n",
       "             b'voted': 29,\n",
       "             b'have_voted': 3,\n",
       "             b'Houten_What': 44,\n",
       "             b'about': 3249,\n",
       "             b'What_about': 175,\n",
       "             b'about_you': 114,\n",
       "             b'you_Bart': 62,\n",
       "             b'Didnt': 65,\n",
       "             b'Bart_Didnt': 1,\n",
       "             b'Didnt_you': 34,\n",
       "             b'you_vote': 9,\n",
       "             b'Simpson_Uh': 542,\n",
       "             b'Wendell': 24,\n",
       "             b'Borton': 15,\n",
       "             b'Wendell_Borton': 15,\n",
       "             b'Yayyyyyyyyyyyyyy': 1,\n",
       "             b'Borton_Yayyyyyyyyyyyyyy': 1,\n",
       "             b'demand': 31,\n",
       "             b'I_demand': 19,\n",
       "             b'demand_a': 6,\n",
       "             b'recount': 3,\n",
       "             b'a_recount': 1,\n",
       "             b'One': 576,\n",
       "             b'KrabappelFlanders_One': 3,\n",
       "             b'One_for': 13,\n",
       "             b'for_Martin': 6,\n",
       "             b'two': 1329,\n",
       "             b'Martin_two': 1,\n",
       "             b'two_for': 5,\n",
       "             b'Would': 235,\n",
       "             b'Martin_Would': 1,\n",
       "             b'Would_you': 178,\n",
       "             b'you_like': 418,\n",
       "             b'another': 609,\n",
       "             b'like_another': 6,\n",
       "             b'another_recount': 1,\n",
       "             b'Simpson_No': 1670,\n",
       "             b'KrabappelFlanders_Well': 48,\n",
       "             b'Well_I': 749,\n",
       "             b'I_just': 831,\n",
       "             b'want': 2669,\n",
       "             b'just_want': 91,\n",
       "             b'want_to': 1159,\n",
       "             b'make': 1870,\n",
       "             b'to_make': 461,\n",
       "             b'make_sure': 90,\n",
       "             b'sure_One': 1,\n",
       "             b'Two': 243,\n",
       "             b'Martin_Two': 1,\n",
       "             b'Two_for': 3,\n",
       "             b'Kid': 118,\n",
       "             b'Reporter': 83,\n",
       "             b'Kid_Reporter': 1,\n",
       "             b'This': 2475,\n",
       "             b'Reporter_This': 1,\n",
       "             b'way': 1625,\n",
       "             b'This_way': 12,\n",
       "             b'Mister': 80,\n",
       "             b'way_Mister': 1,\n",
       "             b'President': 189,\n",
       "             b'Mister_President': 1,\n",
       "             b'Conductor': 20,\n",
       "             b'Conductor_Now': 1,\n",
       "             b'boarding': 13,\n",
       "             b'Now_boarding': 2,\n",
       "             b'on': 7825,\n",
       "             b'boarding_on': 1,\n",
       "             b'track': 55,\n",
       "             b'on_track': 4,\n",
       "             b'5': 22,\n",
       "             b'track_5': 1,\n",
       "             b'5_The': 1,\n",
       "             b'afternoon': 80,\n",
       "             b'The_afternoon': 1,\n",
       "             b'delight': 7,\n",
       "             b'afternoon_delight': 1,\n",
       "             b'coming': 427,\n",
       "             b'delight_coming': 1,\n",
       "             b'coming_to': 51,\n",
       "             b'Shelbyville': 105,\n",
       "             b'to_Shelbyville': 11,\n",
       "             b'Parkville': 1,\n",
       "             b'Shelbyville_Parkville': 1,\n",
       "             b'and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'Parkville_and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'Bergstrom_Hey': 1,\n",
       "             b'Hey_Mr': 18,\n",
       "             b'BERGSTROM': 57,\n",
       "             b'BERGSTROM_Hey': 3,\n",
       "             b'Hey_Lisa': 24,\n",
       "             b'indeed': 32,\n",
       "             b'Lisa_indeed': 1,\n",
       "             b'BERGSTROM_What': 1,\n",
       "             b'What_What': 37,\n",
       "             b'What_is': 211,\n",
       "             b'is_it': 313,\n",
       "             b'Oh': 7634,\n",
       "             b'Simpson_Oh': 3467,\n",
       "             b'Oh_I': 484,\n",
       "             b'mean': 1062,\n",
       "             b'I_mean': 473,\n",
       "             b'were': 2748,\n",
       "             b'mean_were': 6,\n",
       "             b'were_you': 56,\n",
       "             b'you_just': 288,\n",
       "             b'going': 1774,\n",
       "             b'just_going': 31,\n",
       "             b'going_to': 1045,\n",
       "             b'leave': 439,\n",
       "             b'to_leave': 104,\n",
       "             b'leave_just': 2,\n",
       "             b'just_like': 202,\n",
       "             b'like_that': 328,\n",
       "             b'Ah': 785,\n",
       "             b'BERGSTROM_Ah': 2,\n",
       "             b'Im': 9214,\n",
       "             b'Ah_Im': 4,\n",
       "             b'sorry': 894,\n",
       "             b'Im_sorry': 556,\n",
       "             b'sorry_Lisa': 8,\n",
       "             b'You': 6519,\n",
       "             b'Lisa_You': 28,\n",
       "             b'You_know': 897,\n",
       "             b'know_its': 96,\n",
       "             b'its_the': 136,\n",
       "             b'the_life': 36,\n",
       "             b'life_of': 30,\n",
       "             b'of_the': 2174,\n",
       "             b'substitute': 21,\n",
       "             b'the_substitute': 3,\n",
       "             b'teacher': 143,\n",
       "             b'substitute_teacher': 4,\n",
       "             b'teacher_hes': 1,\n",
       "             b'hes_a': 85,\n",
       "             b'fraud': 31,\n",
       "             b'a_fraud': 11,\n",
       "             b'Today': 96,\n",
       "             b'fraud_Today': 1,\n",
       "             b'Today_he': 1,\n",
       "             b'might': 526,\n",
       "             b'he_might': 13,\n",
       "             b'might_be': 121,\n",
       "             b'wearing': 158,\n",
       "             b'be_wearing': 7,\n",
       "             b'gym': 53,\n",
       "             b'wearing_gym': 1,\n",
       "             b'shorts': 43,\n",
       "             b'gym_shorts': 2,\n",
       "             b'tomorrow': 325,\n",
       "             b'shorts_tomorrow': 1,\n",
       "             b'tomorrow_hes': 1,\n",
       "             b'speaking': 44,\n",
       "             b'hes_speaking': 1,\n",
       "             b'French': 123,\n",
       "             b'speaking_French': 2,\n",
       "             b'or': 1564,\n",
       "             b'French_or': 1,\n",
       "             b'or_or': 4,\n",
       "             b'pretending': 21,\n",
       "             b'or_pretending': 1,\n",
       "             b'pretending_to': 13,\n",
       "             b'to_know': 238,\n",
       "             b'know_how': 271,\n",
       "             b'how_to': 217,\n",
       "             b'run': 302,\n",
       "             b'to_run': 43,\n",
       "             b'run_a': 20,\n",
       "             b'band': 102,\n",
       "             b'a_band': 10,\n",
       "             b'saw': 316,\n",
       "             b'band_saw': 1,\n",
       "             b'saw_or': 1,\n",
       "             b'God': 1235,\n",
       "             b'or_God': 2,\n",
       "             b'knows': 199,\n",
       "             b'God_knows': 6,\n",
       "             b'what': 4064,\n",
       "             b'knows_what': 19,\n",
       "             b'Simpson_You': 1811,\n",
       "             b'cant': 2413,\n",
       "             b'You_cant': 213,\n",
       "             b'go': 3162,\n",
       "             b'cant_go': 62,\n",
       "             b'Youre': 1903,\n",
       "             b'go_Youre': 5,\n",
       "             b'Youre_the': 131,\n",
       "             b'best': 643,\n",
       "             b'the_best': 324,\n",
       "             b'best_teacher': 1,\n",
       "             b'Ill': 3093,\n",
       "             b'teacher_Ill': 1,\n",
       "             b'ever': 1209,\n",
       "             b'Ill_ever': 7,\n",
       "             b'ever_have': 21,\n",
       "             b'thats': 1958,\n",
       "             b'Ah_thats': 19,\n",
       "             b'not': 5080,\n",
       "             b'thats_not': 100,\n",
       "             b'true': 356,\n",
       "             b'not_true': 31,\n",
       "             b'Other': 43,\n",
       "             b'true_Other': 1,\n",
       "             b'teachers': 79,\n",
       "             b'Other_teachers': 1,\n",
       "             b'teachers_will': 3,\n",
       "             b'come': 1491,\n",
       "             b'will_come': 25,\n",
       "             b'along': 167,\n",
       "             b'come_along': 9,\n",
       "             b'who': 1755,\n",
       "             b'along_who': 1,\n",
       "             b'please': 857,\n",
       "             b'Oh_please': 52,\n",
       "             b'BERGSTROM_No': 2,\n",
       "             b'No_I': 247,\n",
       "             b'I_cant': 1164,\n",
       "             b'lie': 169,\n",
       "             b'cant_lie': 4,\n",
       "             b'lie_to': 37,\n",
       "             b'to_you': 536,\n",
       "             b'you_I': 220,\n",
       "             b'am': 1428,\n",
       "             b'I_am': 1023,\n",
       "             b'am_the': 53,\n",
       "             b'But': 3513,\n",
       "             b'best_But': 1,\n",
       "             b'But_you': 199,\n",
       "             b'they': 2028,\n",
       "             b'know_they': 20,\n",
       "             b'need': 1528,\n",
       "             b'they_need': 11,\n",
       "             b'me': 10316,\n",
       "             b'need_me': 22,\n",
       "             b'over': 1400,\n",
       "             b'me_over': 16,\n",
       "             b'over_in': 14,\n",
       "             b'in_the': 2893,\n",
       "             b'projects': 15,\n",
       "             b'the_projects': 1,\n",
       "             b'projects_of': 2,\n",
       "             b'of_Capital': 3,\n",
       "             b'Simpson_But': 894,\n",
       "             b'But_I': 511,\n",
       "             b'I_need': 500,\n",
       "             b'need_you': 80,\n",
       "             b'Thats': 2451,\n",
       "             b'BERGSTROM_Thats': 2,\n",
       "             b'Thats_the': 200,\n",
       "             b'problem': 386,\n",
       "             b'the_problem': 53,\n",
       "             b'with': 5933,\n",
       "             b'problem_with': 38,\n",
       "             b'being': 532,\n",
       "             b'with_being': 4,\n",
       "             b'middle': 80,\n",
       "             b'being_middle': 1,\n",
       "             b'class': 264,\n",
       "             b'middle_class': 4,\n",
       "             b'Anybody': 36,\n",
       "             b'class_Anybody': 1,\n",
       "             b'Anybody_who': 2,\n",
       "             b'really': 1599,\n",
       "             b'who_really': 5,\n",
       "             b'cares': 64,\n",
       "             b'really_cares': 2,\n",
       "             b'cares_will': 1,\n",
       "             b'abandon': 11,\n",
       "             b'will_abandon': 1,\n",
       "             b'abandon_you': 3,\n",
       "             b'you_for': 220,\n",
       "             b'those': 1290,\n",
       "             b'for_those': 20,\n",
       "             b'those_who': 24,\n",
       "             b'who_need': 8,\n",
       "             b'need_it': 31,\n",
       "             b'more': 1827,\n",
       "             b'it_more': 15,\n",
       "             b'I_I': 109,\n",
       "             b'understand': 291,\n",
       "             b'I_understand': 76,\n",
       "             b'understand_Mr': 1,\n",
       "             b'Bergstrom_Im': 1,\n",
       "             b'Im_going': 348,\n",
       "             b'miss': 300,\n",
       "             b'to_miss': 18,\n",
       "             b'miss_you': 63,\n",
       "             b'BERGSTROM_Ill': 1,\n",
       "             b'tell': 1257,\n",
       "             b'Ill_tell': 116,\n",
       "             b'tell_you': 373,\n",
       "             b'you_what': 97,\n",
       "             b'Whenever': 21,\n",
       "             b'BERGSTROM_Whenever': 1,\n",
       "             b'Whenever_you': 4,\n",
       "             b'feel': 777,\n",
       "             b'you_feel': 124,\n",
       "             b'feel_like': 139,\n",
       "             b'youre': 3022,\n",
       "             b'like_youre': 56,\n",
       "             b'alone': 251,\n",
       "             b'youre_alone': 5,\n",
       "             b'alone_and': 5,\n",
       "             b'and_theres': 35,\n",
       "             b'nobody': 111,\n",
       "             b'theres_nobody': 6,\n",
       "             b'nobody_you': 1,\n",
       "             b'can': 4187,\n",
       "             b'you_can': 690,\n",
       "             b'rely': 5,\n",
       "             b'can_rely': 1,\n",
       "             b'rely_on': 5,\n",
       "             b'on_this': 208,\n",
       "             b'this_is': 1007,\n",
       "             b'is_all': 125,\n",
       "             b'all_you': 90,\n",
       "             b'you_need': 167,\n",
       "             b'need_to': 316,\n",
       "             b'Thank': 682,\n",
       "             b'Simpson_Thank': 210,\n",
       "             b'Thank_you': 596,\n",
       "             b'you_Mr': 44,\n",
       "             b'All': 1756,\n",
       "             b'Conductor_All': 3,\n",
       "             b'aboard': 31,\n",
       "             b'All_aboard': 8,\n",
       "             b'So': 2035,\n",
       "             b'Simpson_So': 716,\n",
       "             b'So_I': 151,\n",
       "             b'guess': 771,\n",
       "             b'I_guess': 689,\n",
       "             b'guess_this': 36,\n",
       "             b'It': 1568,\n",
       "             b'it_It': 24,\n",
       "             b'It_you': 1,\n",
       "             b'you_dont': 509,\n",
       "             b'mind': 362,\n",
       "             b'dont_mind': 51,\n",
       "             b'mind_Ill': 3,\n",
       "             b'Ill_just': 240,\n",
       "             b'just_run': 3,\n",
       "             b'alongside': 4,\n",
       "             b'run_alongside': 1,\n",
       "             b'alongside_the': 1,\n",
       "             b'the_train': 11,\n",
       "             b'train_as': 1,\n",
       "             b'as_it': 56,\n",
       "             b'speeds': 1,\n",
       "             b'it_speeds': 1,\n",
       "             b'speeds_you': 1,\n",
       "             b'you_from': 56,\n",
       "             b'from_my': 112,\n",
       "             b'my_life': 321,\n",
       "             b'Goodbye': 163,\n",
       "             b'BERGSTROM_Goodbye': 1,\n",
       "             b'Goodbye_Lisa': 8,\n",
       "             b'honey': 454,\n",
       "             b'Lisa_honey': 23,\n",
       "             b'Itll': 79,\n",
       "             b'honey_Itll': 2,\n",
       "             b'Itll_be': 31,\n",
       "             b'okay': 694,\n",
       "             b'be_okay': 36,\n",
       "             b'okay_Just': 6,\n",
       "             b'read': 371,\n",
       "             b'Just_read': 5,\n",
       "             b'read_the': 50,\n",
       "             b'note': 76,\n",
       "             b'the_note': 2,\n",
       "             b'Homer': 32235,\n",
       "             b'Homer_Simpson': 28284,\n",
       "             b'Never': 185,\n",
       "             b'Simpson_Never': 54,\n",
       "             b'thrown': 24,\n",
       "             b'Never_thrown': 1,\n",
       "             b'thrown_a': 3,\n",
       "             b'a_party': 51,\n",
       "             b'party_What': 1,\n",
       "             b'about_that': 175,\n",
       "             b'big': 907,\n",
       "             b'that_big': 27,\n",
       "             b'bash': 12,\n",
       "             b'big_bash': 1,\n",
       "             b'we': 5739,\n",
       "             b'bash_we': 1,\n",
       "             b'had': 1482,\n",
       "             b'we_had': 80,\n",
       "             b'had_with': 1,\n",
       "             b'with_all': 82,\n",
       "             b'champagne': 29,\n",
       "             b'the_champagne': 3,\n",
       "             b'champagne_and': 3,\n",
       "             b'musicians': 6,\n",
       "             b'and_musicians': 1,\n",
       "             b'musicians_and': 2,\n",
       "             b'holy': 33,\n",
       "             b'and_holy': 2,\n",
       "             b'men': 236,\n",
       "             b'holy_men': 1,\n",
       "             b'men_and': 15,\n",
       "             b'everything': 555,\n",
       "             b'and_everything': 20,\n",
       "             b'Simpson_Bart': 726,\n",
       "             b'Bart_didnt': 7,\n",
       "             b'get': 4467,\n",
       "             b'didnt_get': 42,\n",
       "             b'one': 4025,\n",
       "             b'get_one': 32,\n",
       "             b'one_vote': 1,\n",
       "             b'vote_Oh': 1,\n",
       "             b'Oh_this': 80,\n",
       "             b'is_the': 832,\n",
       "             b'worst': 166,\n",
       "             b'the_worst': 120,\n",
       "             b'thing': 1416,\n",
       "             b'worst_thing': 16,\n",
       "             b'thing_that': 60,\n",
       "             b'that_ever': 17,\n",
       "             b'happened': 396,\n",
       "             b'ever_happened': 17,\n",
       "             b'happened_to': 148,\n",
       "             b'us': 2323,\n",
       "             b'to_us': 83,\n",
       "             b'Alright': 43,\n",
       "             b'us_Alright': 1,\n",
       "             b'allright': 1,\n",
       "             b'Alright_allright': 1,\n",
       "             b'spilled': 10,\n",
       "             b'allright_spilled': 1,\n",
       "             b'milk': 113,\n",
       "             b'spilled_milk': 3,\n",
       "             b'milk_spilled': 2,\n",
       "             b'milk_What': 1,\n",
       "             b'are': 5527,\n",
       "             b'What_are': 447,\n",
       "             b'are_you': 1284,\n",
       "             b'you_so': 102,\n",
       "             b'mopey': 1,\n",
       "             b'so_mopey': 1,\n",
       "             b'mopey_about': 1,\n",
       "             b'Nothing': 168,\n",
       "             b'Simpson_Nothing': 72,\n",
       "             b'Marge': 15937,\n",
       "             b'Marge_Simpson': 13289,\n",
       "             b'Simpson_Lisa': 403,\n",
       "             b'Lisa_tell': 5,\n",
       "             b'tell_your': 35,\n",
       "             b'father': 557,\n",
       "             b'your_father': 205,\n",
       "             b'Bergstrom_left': 1,\n",
       "             b'today': 542,\n",
       "             b'left_today': 1,\n",
       "             b'Hes': 955,\n",
       "             b'Simpson_Hes': 192,\n",
       "             b'gone': 416,\n",
       "             b'Hes_gone': 13,\n",
       "             b'Forever': 11,\n",
       "             b'gone_Forever': 1,\n",
       "             b'And': 4992,\n",
       "             b'Simpson_And': 940,\n",
       "             b'didnt_think': 30,\n",
       "             b'youd': 341,\n",
       "             b'think_youd': 18,\n",
       "             b'youd_understand': 2,\n",
       "             b'Hey_just': 5,\n",
       "             b'because': 724,\n",
       "             b'just_because': 56,\n",
       "             b'because_I': 114,\n",
       "             b'care': 465,\n",
       "             b'dont_care': 136,\n",
       "             b'doesnt': 651,\n",
       "             b'care_doesnt': 1,\n",
       "             b'doesnt_mean': 39,\n",
       "             b'mean_I': 56,\n",
       "             b'dont_understand': 88,\n",
       "             b'Simpson_Im': 1276,\n",
       "             b'glad': 218,\n",
       "             b'Im_glad': 115,\n",
       "             b'glad_Im': 8,\n",
       "             b'Im_not': 927,\n",
       "             b'crying': 62,\n",
       "             b'not_crying': 4,\n",
       "             b'crying_because': 2,\n",
       "             b'would': 1757,\n",
       "             b'I_would': 237,\n",
       "             b'hate': 370,\n",
       "             b'would_hate': 3,\n",
       "             b'hate_for': 2,\n",
       "             b'for_you': 625,\n",
       "             b'you_to': 885,\n",
       "             b'to_think': 146,\n",
       "             b'think_that': 73,\n",
       "             b'that_what': 21,\n",
       "             b'what_Im': 93,\n",
       "             b'Im_about': 20,\n",
       "             b'about_to': 144,\n",
       "             b'say_is': 19,\n",
       "             b'based': 42,\n",
       "             b'is_based': 7,\n",
       "             b'based_on': 38,\n",
       "             b'emotion': 7,\n",
       "             b'on_emotion': 1,\n",
       "             b'emotion_But': 1,\n",
       "             b'sir': 832,\n",
       "             b'you_sir': 46,\n",
       "             b'sir_are': 7,\n",
       "             b'are_a': 141,\n",
       "             b'baboon': 9,\n",
       "             b'a_baboon': 4,\n",
       "             b'Me': 295,\n",
       "             b'Simpson_Me': 93,\n",
       "             b'Simpson_Yes': 513,\n",
       "             b'Yes_you': 52,\n",
       "             b'Baboon': 4,\n",
       "             b'you_Baboon': 1,\n",
       "             b'Baboon_baboon': 1,\n",
       "             b'baboon_baboon': 2,\n",
       "             b'realize': 109,\n",
       "             b'you_realize': 31,\n",
       "             b'realize_what': 11,\n",
       "             b'what_youre': 85,\n",
       "             b'saying': 342,\n",
       "             b'youre_saying': 38,\n",
       "             b'Simpson_Baboon': 1,\n",
       "             b'Whoa': 416,\n",
       "             b'Simpson_Whoa': 162,\n",
       "             b'somebody': 116,\n",
       "             b'Whoa_somebody': 1,\n",
       "             b'somebody_was': 1,\n",
       "             b'bound': 19,\n",
       "             b'was_bound': 1,\n",
       "             b'bound_to': 11,\n",
       "             b'say_it': 88,\n",
       "             b'it_one': 12,\n",
       "             b'day': 1269,\n",
       "             b'one_day': 69,\n",
       "             b'day_I': 57,\n",
       "             b'just_cant': 66,\n",
       "             b'believe': 723,\n",
       "             b'cant_believe': 292,\n",
       "             b'believe_it': 90,\n",
       "             b'her': 1728,\n",
       "             b'was_her': 7,\n",
       "             b'Did': 451,\n",
       "             b'Simpson_Did': 163,\n",
       "             b'Did_you': 285,\n",
       "             b'hear': 596,\n",
       "             b'you_hear': 72,\n",
       "             b'hear_that': 70,\n",
       "             b'that_Marge': 22,\n",
       "             b'She': 382,\n",
       "             b'Marge_She': 2,\n",
       "             b'called': 381,\n",
       "             b'She_called': 4,\n",
       "             b'called_me': 22,\n",
       "             b'me_a': 392,\n",
       "             b'baboon_The': 1,\n",
       "             b'stupidest': 16,\n",
       "             b'The_stupidest': 1,\n",
       "             b'ugliest': 6,\n",
       "             b'stupidest_ugliest': 1,\n",
       "             b'smelliest': 1,\n",
       "             b'ugliest_smelliest': 1,\n",
       "             b'ape': 25,\n",
       "             b'smelliest_ape': 1,\n",
       "             b'ape_of': 1,\n",
       "             b'them': 1437,\n",
       "             b'of_them': 149,\n",
       "             b'them_all': 41,\n",
       "             b'Simpson_Homer': 897,\n",
       "             b'Homer_you': 128,\n",
       "             b'you_are': 493,\n",
       "             b'are_not': 79,\n",
       "             b'allowed': 76,\n",
       "             b'not_allowed': 38,\n",
       "             b'allowed_to': 43,\n",
       "             b'to_have': 320,\n",
       "             b'hurt': 280,\n",
       "             b'have_hurt': 2,\n",
       "             b'feelings': 109,\n",
       "             b'hurt_feelings': 3,\n",
       "             b'feelings_right': 1,\n",
       "             b'right_now': 245,\n",
       "             b'Theres': 711,\n",
       "             b'now_Theres': 5,\n",
       "             b'Theres_a': 142,\n",
       "             b'girl': 527,\n",
       "             b'little_girl': 120,\n",
       "             b'upstairs': 30,\n",
       "             b'girl_upstairs': 1,\n",
       "             b'upstairs_who': 1,\n",
       "             b'needs': 263,\n",
       "             b'who_needs': 25,\n",
       "             b'needs_you': 12,\n",
       "             b'Her': 57,\n",
       "             b'you_Her': 1,\n",
       "             b'confidence': 19,\n",
       "             b'Her_confidence': 1,\n",
       "             b'confidence_in': 1,\n",
       "             b'in_her': 38,\n",
       "             b'her_father': 11,\n",
       "             b'father_is': 28,\n",
       "             b'shaken': 5,\n",
       "             b'is_shaken': 1,\n",
       "             b'shaken_and': 1,\n",
       "             b'no': 3459,\n",
       "             b'and_no': 58,\n",
       "             ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(word_tokens, min_count=100, progress_per=5000)\n",
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 10:47:21,455 : INFO : source_vocab length 526421\n",
      "2020-06-09 10:47:29,913 : INFO : Phraser built with 134 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Landlady', 'I', 'think', 'hes', 'taking', 'the', 'next', 'train', 'to', 'Capital', 'City']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agregamos las celdas para entrenar al modelo, para hacer esto utilizamos que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons_word2vec = Word2Vec(\n",
    "    min_count=models_min_count,\n",
    "    window=window_size,\n",
    "    size=model_size,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 10:47:30,026 : INFO : collecting all words and their counts\n",
      "2020-06-09 10:47:30,037 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 10:47:30,896 : INFO : PROGRESS: at sentence #10000, processed 111639 words, keeping 12494 word types\n",
      "2020-06-09 10:47:31,537 : INFO : PROGRESS: at sentence #20000, processed 225335 words, keeping 19455 word types\n",
      "2020-06-09 10:47:32,476 : INFO : PROGRESS: at sentence #30000, processed 348717 words, keeping 25712 word types\n",
      "2020-06-09 10:47:33,473 : INFO : PROGRESS: at sentence #40000, processed 463626 words, keeping 30091 word types\n",
      "2020-06-09 10:47:34,436 : INFO : PROGRESS: at sentence #50000, processed 570988 words, keeping 34183 word types\n",
      "2020-06-09 10:47:34,871 : INFO : PROGRESS: at sentence #60000, processed 672231 words, keeping 37740 word types\n",
      "2020-06-09 10:47:35,493 : INFO : PROGRESS: at sentence #70000, processed 783747 words, keeping 41547 word types\n",
      "2020-06-09 10:47:36,394 : INFO : PROGRESS: at sentence #80000, processed 901213 words, keeping 45285 word types\n",
      "2020-06-09 10:47:37,348 : INFO : PROGRESS: at sentence #90000, processed 1016338 words, keeping 48725 word types\n",
      "2020-06-09 10:47:37,981 : INFO : PROGRESS: at sentence #100000, processed 1132124 words, keeping 51821 word types\n",
      "2020-06-09 10:47:38,489 : INFO : PROGRESS: at sentence #110000, processed 1250144 words, keeping 55247 word types\n",
      "2020-06-09 10:47:38,986 : INFO : PROGRESS: at sentence #120000, processed 1365616 words, keeping 58198 word types\n",
      "2020-06-09 10:47:39,473 : INFO : PROGRESS: at sentence #130000, processed 1480659 words, keeping 60431 word types\n",
      "2020-06-09 10:47:39,568 : INFO : collected 60799 word types from a corpus of 1501787 raw words and 131853 sentences\n",
      "2020-06-09 10:47:39,569 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 10:47:39,728 : INFO : effective_min_count=10 retains 8961 unique words (14% of original 60799, drops 51838)\n",
      "2020-06-09 10:47:39,730 : INFO : effective_min_count=10 leaves 1389630 word corpus (92% of original 1501787, drops 112157)\n",
      "2020-06-09 10:47:39,776 : INFO : deleting the raw counts dictionary of 60799 items\n",
      "2020-06-09 10:47:39,778 : INFO : sample=6e-05 downsamples 756 most-common words\n",
      "2020-06-09 10:47:39,780 : INFO : downsampling leaves estimated 578086 word corpus (41.6% of prior 1389630)\n",
      "2020-06-09 10:47:39,817 : INFO : estimated required memory for 8961 words and 200 dimensions: 18818100 bytes\n",
      "2020-06-09 10:47:39,819 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# construyo el vocabulario\n",
    "simpsons_word2vec.build_vocab(sentences, progress_per=vocab_progress_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:13:37,966 : INFO : training model with 4 workers on 8961 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
      "2020-06-09 11:13:39,004 : INFO : EPOCH 1 - PROGRESS: at 14.22% examples, 78895 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:13:47,033 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:13:47,036 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:13:47,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:13:47,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:13:47,095 : INFO : EPOCH - 1 : training on 1501787 raw words (577907 effective words) took 9.1s, 63407 effective words/s\n",
      "2020-06-09 11:13:48,135 : INFO : EPOCH 2 - PROGRESS: at 14.22% examples, 78875 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:13:55,290 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:13:55,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:13:55,299 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:13:55,318 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:13:55,320 : INFO : EPOCH - 2 : training on 1501787 raw words (578089 effective words) took 8.2s, 70389 effective words/s\n",
      "2020-06-09 11:13:56,371 : INFO : EPOCH 3 - PROGRESS: at 13.60% examples, 74300 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:03,022 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:03,023 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:03,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:03,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:03,040 : INFO : EPOCH - 3 : training on 1501787 raw words (578209 effective words) took 7.7s, 75025 effective words/s\n",
      "2020-06-09 11:14:04,086 : INFO : EPOCH 4 - PROGRESS: at 12.98% examples, 72307 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:10,310 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:10,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:10,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:10,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:10,343 : INFO : EPOCH - 4 : training on 1501787 raw words (577822 effective words) took 7.3s, 79504 effective words/s\n",
      "2020-06-09 11:14:11,386 : INFO : EPOCH 5 - PROGRESS: at 12.98% examples, 72702 words/s, in_qsize 1, out_qsize 0\n",
      "2020-06-09 11:14:18,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:18,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:18,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:18,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:18,687 : INFO : EPOCH - 5 : training on 1501787 raw words (578991 effective words) took 8.3s, 69669 effective words/s\n",
      "2020-06-09 11:14:19,714 : INFO : EPOCH 6 - PROGRESS: at 12.98% examples, 72460 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:25,947 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:25,949 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:25,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:25,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:25,975 : INFO : EPOCH - 6 : training on 1501787 raw words (578735 effective words) took 7.3s, 79579 effective words/s\n",
      "2020-06-09 11:14:27,010 : INFO : EPOCH 7 - PROGRESS: at 14.22% examples, 78632 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:32,978 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:32,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:32,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:33,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:33,011 : INFO : EPOCH - 7 : training on 1501787 raw words (577720 effective words) took 7.0s, 82228 effective words/s\n",
      "2020-06-09 11:14:34,029 : INFO : EPOCH 8 - PROGRESS: at 14.22% examples, 80329 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:41,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:41,352 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:41,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:41,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:41,403 : INFO : EPOCH - 8 : training on 1501787 raw words (578502 effective words) took 8.4s, 69019 effective words/s\n",
      "2020-06-09 11:14:42,520 : INFO : EPOCH 9 - PROGRESS: at 10.40% examples, 52686 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:49,429 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:49,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:49,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:49,453 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:49,454 : INFO : EPOCH - 9 : training on 1501787 raw words (577879 effective words) took 8.0s, 71919 effective words/s\n",
      "2020-06-09 11:14:50,508 : INFO : EPOCH 10 - PROGRESS: at 14.22% examples, 78000 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:14:57,143 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:14:57,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:14:57,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:14:57,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:14:57,170 : INFO : EPOCH - 10 : training on 1501787 raw words (577816 effective words) took 7.7s, 75050 effective words/s\n",
      "2020-06-09 11:14:58,194 : INFO : EPOCH 11 - PROGRESS: at 12.37% examples, 68061 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:15:04,536 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:15:04,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:15:04,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:15:04,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:15:04,560 : INFO : EPOCH - 11 : training on 1501787 raw words (578268 effective words) took 7.4s, 78340 effective words/s\n",
      "2020-06-09 11:15:05,616 : INFO : EPOCH 12 - PROGRESS: at 14.81% examples, 81538 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:15:11,579 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:15:11,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:15:11,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:15:11,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:15:11,603 : INFO : EPOCH - 12 : training on 1501787 raw words (577718 effective words) took 7.0s, 82275 effective words/s\n",
      "2020-06-09 11:15:12,635 : INFO : EPOCH 13 - PROGRESS: at 12.98% examples, 72264 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:15:18,714 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:15:18,716 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:15:18,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:15:18,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:15:18,744 : INFO : EPOCH - 13 : training on 1501787 raw words (578498 effective words) took 7.1s, 81210 effective words/s\n",
      "2020-06-09 11:15:19,760 : INFO : EPOCH 14 - PROGRESS: at 14.22% examples, 80112 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:15:26,579 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:15:26,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:15:26,589 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:15:26,609 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:15:26,610 : INFO : EPOCH - 14 : training on 1501787 raw words (578428 effective words) took 7.9s, 73625 effective words/s\n",
      "2020-06-09 11:15:27,636 : INFO : EPOCH 15 - PROGRESS: at 12.98% examples, 73030 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:15:33,914 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:15:33,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:15:33,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:15:33,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:15:33,928 : INFO : EPOCH - 15 : training on 1501787 raw words (577893 effective words) took 7.3s, 79238 effective words/s\n",
      "2020-06-09 11:15:33,930 : INFO : training on a 22526805 raw words (8672475 effective words) took 116.0s, 74790 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8672475, 22526805)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_word2vec.train(sentences, total_examples=simpsons_word2vec.corpus_count, epochs=train_epochs, report_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:17:17,198 : INFO : saving Word2Vec object under ./pretrained_models/simpsons_word2vec.model, separately None\n",
      "2020-06-09 11:17:17,202 : INFO : not storing attribute vectors_norm\n",
      "2020-06-09 11:17:17,206 : INFO : not storing attribute cum_table\n",
      "2020-06-09 11:17:17,469 : INFO : saved ./pretrained_models/simpsons_word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo\n",
    "\n",
    "if not os.path.exists('./pretrained_models/simpsons_word2vec.model'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "simpsons_word2vec.save('./pretrained_models/simpsons_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 16:51:58,194 : INFO : loading Word2VecKeyedVectors object from ./pretrained_models/simpsons_word2vec.model\n",
      "2020-06-09 16:51:58,394 : INFO : loading wv recursively from ./pretrained_models/simpsons_word2vec.model.wv.* with mmap=r\n",
      "2020-06-09 16:51:58,394 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-06-09 16:51:58,395 : INFO : loading vocabulary recursively from ./pretrained_models/simpsons_word2vec.model.vocabulary.* with mmap=r\n",
      "2020-06-09 16:51:58,396 : INFO : loading trainables recursively from ./pretrained_models/simpsons_word2vec.model.trainables.* with mmap=r\n",
      "2020-06-09 16:51:58,397 : INFO : setting ignored attribute cum_table to None\n",
      "2020-06-09 16:51:58,398 : INFO : loaded ./pretrained_models/simpsons_word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "simpsons_word2vec = KeyedVectors.load('./pretrained_models/simpsons_word2vec.model',mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:41:30,826 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_fast_text = FastText(size=model_size, window=window_size, min_count=models_min_count, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:42:07,646 : INFO : collecting all words and their counts\n",
      "2020-06-09 11:42:07,696 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 11:42:08,562 : INFO : PROGRESS: at sentence #10000, processed 111639 words, keeping 12494 word types\n",
      "2020-06-09 11:42:09,250 : INFO : PROGRESS: at sentence #20000, processed 225335 words, keeping 19455 word types\n",
      "2020-06-09 11:42:09,850 : INFO : PROGRESS: at sentence #30000, processed 348717 words, keeping 25712 word types\n",
      "2020-06-09 11:42:10,351 : INFO : PROGRESS: at sentence #40000, processed 463626 words, keeping 30091 word types\n",
      "2020-06-09 11:42:10,827 : INFO : PROGRESS: at sentence #50000, processed 570988 words, keeping 34183 word types\n",
      "2020-06-09 11:42:11,293 : INFO : PROGRESS: at sentence #60000, processed 672231 words, keeping 37740 word types\n",
      "2020-06-09 11:42:11,787 : INFO : PROGRESS: at sentence #70000, processed 783747 words, keeping 41547 word types\n",
      "2020-06-09 11:42:12,311 : INFO : PROGRESS: at sentence #80000, processed 901213 words, keeping 45285 word types\n",
      "2020-06-09 11:42:12,830 : INFO : PROGRESS: at sentence #90000, processed 1016338 words, keeping 48725 word types\n",
      "2020-06-09 11:42:13,520 : INFO : PROGRESS: at sentence #100000, processed 1132124 words, keeping 51821 word types\n",
      "2020-06-09 11:42:14,084 : INFO : PROGRESS: at sentence #110000, processed 1250144 words, keeping 55247 word types\n",
      "2020-06-09 11:42:14,638 : INFO : PROGRESS: at sentence #120000, processed 1365616 words, keeping 58198 word types\n",
      "2020-06-09 11:42:15,210 : INFO : PROGRESS: at sentence #130000, processed 1480659 words, keeping 60431 word types\n",
      "2020-06-09 11:42:15,318 : INFO : collected 60799 word types from a corpus of 1501787 raw words and 131853 sentences\n",
      "2020-06-09 11:42:15,319 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 11:42:16,637 : INFO : effective_min_count=10 retains 8961 unique words (14% of original 60799, drops 51838)\n",
      "2020-06-09 11:42:16,638 : INFO : effective_min_count=10 leaves 1389630 word corpus (92% of original 1501787, drops 112157)\n",
      "2020-06-09 11:42:16,676 : INFO : deleting the raw counts dictionary of 60799 items\n",
      "2020-06-09 11:42:16,679 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2020-06-09 11:42:16,684 : INFO : downsampling leaves estimated 1074177 word corpus (77.3% of prior 1389630)\n",
      "2020-06-09 11:42:16,839 : INFO : estimated required memory for 8961 words, 63219 buckets and 200 dimensions: 71084180 bytes\n",
      "2020-06-09 11:42:16,842 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_fast_text.build_vocab(sentences=sentences, progress_per=vocab_progress_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:42:35,093 : INFO : training model with 4 workers on 8961 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2020-06-09 11:42:36,131 : INFO : EPOCH 1 - PROGRESS: at 8.23% examples, 84696 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:42:44,555 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:42:44,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:42:44,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:42:44,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:42:44,646 : INFO : EPOCH - 1 : training on 1501787 raw words (1074122 effective words) took 9.5s, 112764 effective words/s\n",
      "2020-06-09 11:42:45,675 : INFO : EPOCH 2 - PROGRESS: at 10.40% examples, 105904 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:42:54,899 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:42:54,941 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:42:54,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:42:55,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:42:55,014 : INFO : EPOCH - 2 : training on 1501787 raw words (1074571 effective words) took 10.3s, 103824 effective words/s\n",
      "2020-06-09 11:42:56,077 : INFO : EPOCH 3 - PROGRESS: at 9.71% examples, 95866 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:05,160 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:05,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:05,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:43:05,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:43:05,268 : INFO : EPOCH - 3 : training on 1501787 raw words (1074165 effective words) took 10.2s, 104960 effective words/s\n",
      "2020-06-09 11:43:06,303 : INFO : EPOCH 4 - PROGRESS: at 10.40% examples, 104654 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:15,363 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:15,391 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:15,413 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:43:15,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:43:15,479 : INFO : EPOCH - 4 : training on 1501787 raw words (1074314 effective words) took 10.2s, 105324 effective words/s\n",
      "2020-06-09 11:43:16,548 : INFO : EPOCH 5 - PROGRESS: at 6.73% examples, 67427 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:25,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:25,203 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:25,217 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:43:25,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:43:25,276 : INFO : EPOCH - 5 : training on 1501787 raw words (1074198 effective words) took 9.8s, 109764 effective words/s\n",
      "2020-06-09 11:43:26,400 : INFO : EPOCH 6 - PROGRESS: at 9.71% examples, 89737 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:35,078 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:35,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:35,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:43:35,184 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:43:35,185 : INFO : EPOCH - 6 : training on 1501787 raw words (1073863 effective words) took 9.9s, 108486 effective words/s\n",
      "2020-06-09 11:43:36,194 : INFO : EPOCH 7 - PROGRESS: at 10.40% examples, 107375 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:45,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:45,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:45,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:43:45,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:43:45,632 : INFO : EPOCH - 7 : training on 1501787 raw words (1074202 effective words) took 10.4s, 102918 effective words/s\n",
      "2020-06-09 11:43:46,716 : INFO : EPOCH 8 - PROGRESS: at 5.33% examples, 53794 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:56,727 : INFO : EPOCH 8 - PROGRESS: at 84.02% examples, 81332 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:43:59,900 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:43:59,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:43:59,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:44:00,000 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:44:00,001 : INFO : EPOCH - 8 : training on 1501787 raw words (1074672 effective words) took 14.4s, 74883 effective words/s\n",
      "2020-06-09 11:44:01,060 : INFO : EPOCH 9 - PROGRESS: at 3.34% examples, 34190 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:11,126 : INFO : EPOCH 9 - PROGRESS: at 43.84% examples, 41929 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:18,133 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:44:18,134 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:44:18,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:44:18,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:44:18,224 : INFO : EPOCH - 9 : training on 1501787 raw words (1074873 effective words) took 18.2s, 59020 effective words/s\n",
      "2020-06-09 11:44:19,310 : INFO : EPOCH 10 - PROGRESS: at 8.23% examples, 80393 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:29,459 : INFO : EPOCH 10 - PROGRESS: at 81.41% examples, 77817 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:32,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:44:32,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:44:32,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:44:32,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:44:32,203 : INFO : EPOCH - 10 : training on 1501787 raw words (1074899 effective words) took 14.0s, 77003 effective words/s\n",
      "2020-06-09 11:44:33,249 : INFO : EPOCH 11 - PROGRESS: at 10.40% examples, 104090 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:42,911 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:44:42,949 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:44:42,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:44:43,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:44:43,026 : INFO : EPOCH - 11 : training on 1501787 raw words (1074618 effective words) took 10.8s, 99439 effective words/s\n",
      "2020-06-09 11:44:44,048 : INFO : EPOCH 12 - PROGRESS: at 10.40% examples, 105801 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:54,350 : INFO : EPOCH 12 - PROGRESS: at 95.16% examples, 90338 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:44:55,047 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:44:55,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:44:55,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:44:55,205 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:44:55,206 : INFO : EPOCH - 12 : training on 1501787 raw words (1073898 effective words) took 12.2s, 88232 effective words/s\n",
      "2020-06-09 11:44:56,294 : INFO : EPOCH 13 - PROGRESS: at 2.65% examples, 26566 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:45:06,357 : INFO : EPOCH 13 - PROGRESS: at 57.97% examples, 55237 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:45:10,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:45:10,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:45:10,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:45:10,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:45:10,445 : INFO : EPOCH - 13 : training on 1501787 raw words (1074600 effective words) took 15.2s, 70558 effective words/s\n",
      "2020-06-09 11:45:11,469 : INFO : EPOCH 14 - PROGRESS: at 4.01% examples, 42312 words/s, in_qsize 1, out_qsize 0\n",
      "2020-06-09 11:45:21,070 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:45:21,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:45:21,108 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:45:21,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:45:21,173 : INFO : EPOCH - 14 : training on 1501787 raw words (1073952 effective words) took 10.7s, 100216 effective words/s\n",
      "2020-06-09 11:45:22,238 : INFO : EPOCH 15 - PROGRESS: at 11.09% examples, 108492 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:45:30,239 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:45:30,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:45:30,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:45:30,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:45:30,328 : INFO : EPOCH - 15 : training on 1501787 raw words (1074225 effective words) took 9.1s, 117480 effective words/s\n",
      "2020-06-09 11:45:30,329 : INFO : training on a 22526805 raw words (16115172 effective words) took 175.2s, 91969 effective words/s\n"
     ]
    }
   ],
   "source": [
    "simpsons_fast_text.train(sentences,total_examples=simpsons_fast_text.corpus_count, epochs=train_epochs, report_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:46:02,235 : INFO : saving FastText object under ./pretrained_models/simpsons_fast_text.model, separately None\n",
      "2020-06-09 11:46:02,237 : INFO : storing np array 'vectors_ngrams' to ./pretrained_models/simpsons_fast_text.model.wv.vectors_ngrams.npy\n",
      "2020-06-09 11:46:07,687 : INFO : not storing attribute vectors_norm\n",
      "2020-06-09 11:46:07,688 : INFO : not storing attribute vectors_vocab_norm\n",
      "2020-06-09 11:46:07,689 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2020-06-09 11:46:07,690 : INFO : not storing attribute buckets_word\n",
      "2020-06-09 11:46:07,692 : INFO : storing np array 'vectors_ngrams_lockf' to ./pretrained_models/simpsons_fast_text.model.trainables.vectors_ngrams_lockf.npy\n",
      "2020-06-09 11:46:12,584 : INFO : saved ./pretrained_models/simpsons_fast_text.model\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo\n",
    "if not os.path.exists('./pretrained_models/'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "simpsons_fast_text.save('./pretrained_models/simpsons_fast_text.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 16:51:43,899 : INFO : loading Word2VecKeyedVectors object from ./pretrained_models/simpsons_fast_text.model\n",
      "2020-06-09 16:51:44,190 : INFO : loading wv recursively from ./pretrained_models/simpsons_fast_text.model.wv.* with mmap=r\n",
      "2020-06-09 16:51:44,191 : INFO : loading vectors_ngrams from ./pretrained_models/simpsons_fast_text.model.wv.vectors_ngrams.npy with mmap=r\n",
      "2020-06-09 16:51:44,208 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-06-09 16:51:44,210 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2020-06-09 16:51:44,220 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-06-09 16:51:44,232 : INFO : setting ignored attribute buckets_word to None\n",
      "2020-06-09 16:51:44,251 : INFO : loading vocabulary recursively from ./pretrained_models/simpsons_fast_text.model.vocabulary.* with mmap=r\n",
      "2020-06-09 16:51:44,261 : INFO : loading trainables recursively from ./pretrained_models/simpsons_fast_text.model.trainables.* with mmap=r\n",
      "2020-06-09 16:51:44,265 : INFO : loading vectors_ngrams_lockf from ./pretrained_models/simpsons_fast_text.model.trainables.vectors_ngrams_lockf.npy with mmap=r\n",
      "2020-06-09 16:51:44,270 : INFO : loaded ./pretrained_models/simpsons_fast_text.model\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "simpsons_fast_text = KeyedVectors.load('./pretrained_models/simpsons_fast_text.model',mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oos6kUF5tYJr"
   },
   "source": [
    "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. Cúal es la diferencia entre ambos resultados? Por qué ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escogería uno vs el otro? (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wqX03jStYJr"
   },
   "source": [
    "**Respuesta**: Veamos primero las palabras similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3ovjW6WtYJr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:47:15,023 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Bart', 0.8083334565162659),\n",
       " ('Dad', 0.7887272834777832),\n",
       " ('Mom', 0.7790639996528625),\n",
       " ('Simpson', 0.6868166923522949),\n",
       " ('Marge', 0.6677660942077637),\n",
       " ('Adult', 0.6459310054779053),\n",
       " ('JANEY', 0.6249232292175293),\n",
       " ('Jessica', 0.622282862663269),\n",
       " ('Lis', 0.6137495040893555),\n",
       " ('Miss_Hoover', 0.6135748624801636)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_word2vec.wv.most_similar(positive=['Lisa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:47:43,787 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-06-09 11:47:43,813 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Bart', 0.8728782534599304),\n",
       " ('Homer', 0.7914650440216064),\n",
       " ('Bartdude', 0.767725944519043),\n",
       " ('Marge', 0.7653396725654602),\n",
       " ('Lis', 0.7518231868743896),\n",
       " ('Bartman', 0.740960955619812),\n",
       " ('Lisas', 0.733505368232727),\n",
       " ('Grampa', 0.7192960977554321),\n",
       " ('Homey', 0.6765779256820679),\n",
       " ('Bartender', 0.6699763536453247)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast_text.wv.most_similar(positive=['Lisa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisa', 0.8083334565162659),\n",
       " ('Lis', 0.805853545665741),\n",
       " ('Mom', 0.7695565223693848),\n",
       " ('Dad', 0.7516729235649109),\n",
       " ('Jimbo_Jones', 0.7449358701705933),\n",
       " ('Nelson_Muntz', 0.7128869295120239),\n",
       " ('Adult', 0.7124850153923035),\n",
       " ('DOLPH', 0.6949911117553711),\n",
       " ('Jessica', 0.6936681866645813),\n",
       " ('JANEY', 0.6913785338401794)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_word2vec.wv.most_similar(positive=['Bart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bartdude', 0.877600371837616),\n",
       " ('Lisa', 0.8728783130645752),\n",
       " ('Homer', 0.8016128540039062),\n",
       " ('Bartman', 0.7865314483642578),\n",
       " ('Bar', 0.7600126266479492),\n",
       " ('Grampa', 0.7599841356277466),\n",
       " ('Bartholomew', 0.7527804970741272),\n",
       " ('Bartender', 0.7407093644142151),\n",
       " ('Mozart', 0.7369310855865479),\n",
       " ('Barbara', 0.7366133332252502)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast_text.wv.most_similar(positive=['Bart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Selma_Bouvier', 0.7662931680679321),\n",
       " ('Marge', 0.7648098468780518),\n",
       " ('Grampa', 0.7560476064682007),\n",
       " ('Simpson', 0.7466959953308105),\n",
       " ('Lenny_Leonard', 0.7068732976913452),\n",
       " ('HERB', 0.6873685121536255),\n",
       " ('Barney_Gumble', 0.6852911710739136),\n",
       " ('BUCK', 0.6751337051391602),\n",
       " ('Patty_Bouvier', 0.6743243932723999),\n",
       " ('Carl_Carlson', 0.6704251170158386)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_word2vec.wv.most_similar(positive=['Homer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grampa', 0.8626407384872437),\n",
       " ('Homey', 0.8553187251091003),\n",
       " ('Bart', 0.8016128540039062),\n",
       " ('Homers', 0.7996982336044312),\n",
       " ('Home', 0.7975268363952637),\n",
       " ('Marge', 0.7940696477890015),\n",
       " ('Lisa', 0.7914650440216064),\n",
       " ('Grampas', 0.7763826847076416),\n",
       " ('Homeless', 0.7483166456222534),\n",
       " ('Gramma', 0.725990355014801)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast_text.wv.most_similar(positive=['Homer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Homie', 0.8502767086029053),\n",
       " ('honey', 0.7772771716117859),\n",
       " ('Selma_Bouvier', 0.7691065073013306),\n",
       " ('Homer', 0.7648098468780518),\n",
       " ('Simpson', 0.716396689414978),\n",
       " ('sweetie', 0.7131199240684509),\n",
       " ('Manjula_Nahasapeemapetilon', 0.7095891237258911),\n",
       " ('your_father', 0.6992154121398926),\n",
       " ('HUGH', 0.6959747076034546),\n",
       " ('Helen_Lovejoy', 0.6949127912521362)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_word2vec.wv.most_similar(positive=['Marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marges', 0.8739074468612671),\n",
       " ('Margarine', 0.8211492896080017),\n",
       " ('Homer', 0.7940696477890015),\n",
       " ('Marjorie', 0.7748557329177856),\n",
       " ('Lisa', 0.7653396725654602),\n",
       " ('Grampa', 0.7511587142944336),\n",
       " ('Mary', 0.7331581115722656),\n",
       " ('Marco', 0.7325822114944458),\n",
       " ('Maude_Flanders', 0.6983836889266968),\n",
       " ('Martha', 0.6905038356781006)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast_text.wv.most_similar(positive=['Marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre ambos radica en que word2vec recoge que palabras son utilizadas en el mismo contexto, es decir hace una comparación entre palabras. Mientras que podemos observar que fast_text encuentra que palabras son más parecidas entre sus letras, por ejemplo, al buscar `Homer` word2vec ve que son `Selma_Bouvier`, `Marge`, `Grampa` que son personajes propios de los simpsons y puede haberlas dicho en sus dialogos. En cambio al aplicar el embedding de fast_text en la busqueda de la misma palabra se tiene que los resultados mas parecidos son `Grampa`, `Homey`, `Bart`, `Homers`, que son palabras relacionadas o bien formas distintas de referirse al mismo persona. Esta diferencia se puede deber a que la task que inventada que tratan resolver es distinta, a pesar de utilizar el mismo tokenizador. Uno puede estar realizando una task sobre los ngramas de palabras (word2vec) y el otro sobre ngramas de caracteres (fast_text) para representar el corpus y asi entrenar los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Liisa' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8711a71eb65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimpsons_word2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Liisa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Liisa' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "simpsons_word2vec.wv.most_similar(positive=['Liisa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisa', 0.978600263595581),\n",
       " ('Bart', 0.846942663192749),\n",
       " ('Bartdude', 0.7820175886154175),\n",
       " ('Homer', 0.7789160013198853),\n",
       " ('Marge', 0.7479390501976013),\n",
       " ('Bartman', 0.7419960498809814),\n",
       " ('Lis', 0.7318328022956848),\n",
       " ('Lisas', 0.7130515575408936),\n",
       " ('Bar', 0.7047368288040161),\n",
       " ('Marjorie', 0.7000263929367065)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast_text.wv.most_similar(positive=['Liisa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia es notable, en word2vec se tiene que este no puede encontrar palabras relacionadas con `Liisa` pues no esta en el vocabulario. Pero fast_text tiene que la palabra màs relacionada es `Lisa` bien escrita, lo que confirmaria que la representación del vocabulario para ser entrenado el bag of word es distinta.\n",
    "\n",
    "word2vec seria bueno para task sobre clasificacion de texto que sean beneficiados al extraer los ngramas de palabras del vocabulario, por ejemplo, la del ejercicio 1, que fue clasificar noticias en 4 categorias del mismo idioma. fast_text es util para modelos de correción de escritura (como el que puede utilizar nuestro celular al tipear sobre el o google para recomendar otra busqueda al equivocarnos en tipear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAbldwNFtYJu"
   },
   "source": [
    "### Parte 4: Aplicar embeddings para clasificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5DlKSestYJu"
   },
   "source": [
    "Ahora utilizaremos estos embeddings para clasificar palabras basadas en su polaridad (positivas o negativas). Para esto ocuparemos el lexicón AFINN incluido en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCDFYm_ytYJu"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tops</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groan</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perfects</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spammer</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saluting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1\n",
       "0      tops  1\n",
       "1     groan -1\n",
       "2  perfects  1\n",
       "3   spammer -1\n",
       "4  saluting  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_afinn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-icASpvytYJw"
   },
   "source": [
    "Hint: Para w2v son esperables KeyErrors, para eso pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar), para luego aplicar los embeddings en toda la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXw9CGmgtYJx"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8d8qtIgtYJy"
   },
   "source": [
    "**Pregunta 1**: Una vez que tengan un dataframe del estilo [embedding, sentimiento] para ambos modelos, separarlo utilizando la siguiente función, donde X es su columna de embeddings e y es la columna de los valores. (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Dw6KBAftYJ1"
   },
   "source": [
    "**Respuesta**: Construyo lo siguiente funcion que entrega tales dataframe de pandas del estilo `[embeddings..., sentimiento]` donde `embeddings...` es la expansion del vector de embedding de esa frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Construccion del dataset\n",
    "def get_dataset_with_embeddings(embedding_model) -> Tuple[pd.DataFrame]:\n",
    "    dataset = []\n",
    "    for row in df_afinn.iterrows():\n",
    "        word = row[1][0]\n",
    "        word_class = row[1][1]\n",
    "        if try_apply(model=embedding_model,word=word):\n",
    "            dataset.append([*embedding_model[word],word_class])\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset.set_axis([*dataset.columns[:-1],'feeling'],axis=1,inplace=True)\n",
    "    return dataset.loc[:, dataset.columns != 'feeling'], dataset['feeling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sacando estas representaciones para los dos embeddings tenemos que:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_embeddings,fast_text_target = get_dataset_with_embeddings(simpsons_fast_text.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec_embeddings, word_2_vec_target = get_dataset_with_embeddings(simpsons_word2vec.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_frb5aDatYJz"
   },
   "outputs": [],
   "source": [
    "fast_text_train, fast_text_test, fast_text_prediccions_train, fast_text_prediccions_test = train_test_split(fast_text_embeddings, fast_text_target, random_state=0, test_size=0.1, stratify=fast_text_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmrrkpaStYJ1"
   },
   "outputs": [],
   "source": [
    "word_2_vec_train, word_2_vec_test, word_2_vec_prediccions_train, word_2_vec_prediccions_test = train_test_split(word_2_vec_embeddings, word_2_vec_target, random_state=0, test_size=0.1, stratify=word_2_vec_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u33__LaNtYJ3"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dP5zYH3qtYJ3"
   },
   "source": [
    "**Respuesta**: Primero definamos una funcion para entrenar una regression logistica sobre el dataset correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(X_train: pd.DataFrame, Y_train: pd.DataFrame) -> linear_model.LogisticRegression:\n",
    "    classifier = linear_model.LogisticRegression(max_iter=1000000)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos los dos modelos que nos piden, partiendo por el fast text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_classifier = train_linear_model(fast_text_train, fast_text_prediccions_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos sus metricas pedidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_test_pred = fast_text_classifier.predict(fast_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix using fast text embeddings\n",
      "[[192  29]\n",
      " [ 72  46]]\n",
      "Classification report using fast text embeddings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.87      0.79       221\n",
      "           1       0.61      0.39      0.48       118\n",
      "\n",
      "    accuracy                           0.70       339\n",
      "   macro avg       0.67      0.63      0.63       339\n",
      "weighted avg       0.69      0.70      0.68       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf_matrix = confusion_matrix(fast_text_prediccions_test, fast_text_test_pred)\n",
    "print(\"Confussion matrix using fast text embeddings\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification report using fast text embeddings\")\n",
    "print(classification_report(fast_text_prediccions_test, fast_text_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec_classifier = train_linear_model(word_2_vec_train, word_2_vec_prediccions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec_pred = word_2_vec_classifier.predict(word_2_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix using word2vec embeddings\n",
      "[[45  7]\n",
      " [22 17]]\n",
      "Classification report using word2vec embeddings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.87      0.76        52\n",
      "           1       0.71      0.44      0.54        39\n",
      "\n",
      "    accuracy                           0.68        91\n",
      "   macro avg       0.69      0.65      0.65        91\n",
      "weighted avg       0.69      0.68      0.66        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_2_vec_conf_matrix = confusion_matrix(word_2_vec_prediccions_test, word_2_vec_pred)\n",
    "print(\"Confussion matrix using word2vec embeddings\")\n",
    "print(word_2_vec_conf_matrix)\n",
    "print(\"Classification report using word2vec embeddings\")\n",
    "print(classification_report(word_2_vec_prediccions_test, word_2_vec_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos son buenos pues se esta entrenando en base a la representacion del dataset de los simpson las palabras parecidas con el dataset de entrenamiento proporcionado. Ahora bien se esperaban mejores resultados en word2vec que en fast_text pero fue todo lo contrario, esto se puede verpues no todas las palabras que estaban en el vocabulario con que se entrenó word2vec estan en el dataset en el que se queria utilizar. Por tanto se puede entrenar a los embeddings sobre un dataset más grande, pues el que se proveyó sobre los dialogos de los simpsons puede ser muy pequeño para este tipo de tareas.\n",
    "\n",
    "\n",
    "Ademas se pueden generar aun mejores predicciones, pues los features en cual el modelo se esta entrenando son los embeddings directamente, y en vez de entrenar el modelo lineal con estos embeddings podriamos aplicarles una operacion al embedding. Por ejemplo, obtener el maximo y entrenar a partir del maximo valor del embedding y la preddiccion, o bien sacar el promedio de los valores en sus componentes y predecir en base a tal feature en vez de todo el embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOt6N4nftYJ4"
   },
   "source": [
    "# Bonus: 2 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnsV0J6StYJ4"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim (1 punto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lYudsa_tYJ4"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd4psA7LtYJ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrZxZZfutYJ7"
   },
   "source": [
    "**Pregunta 2**: Utilizar wefe para ver si el modelo w2v entrenado con los dialogos de los Simpson tienen algun bias entre los personajes hombres y la cerveza (1 punto):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Subk47EatYJ7"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgThotyEtYJ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minitarea3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
