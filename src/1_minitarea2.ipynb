{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2obO44rRIDIm"
   },
   "source": [
    "# Minitarea 2\n",
    "\n",
    "Nombre: Joaquín Cruz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JstKx3TiKcIp"
   },
   "source": [
    "---------------------------\n",
    "## Language Models (3 pts)\n",
    "Estas preguntas son teóricas y deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas. El material del curso y los videos deberian ser suficientes para que puedan responder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hwW-07MrRpt"
   },
   "source": [
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Asuma que tiene un modelo de lenguaje de trigramas (simple) entrenado sobre un corpus C. Muestre cómo el modelo le asigna una probabilidad a la oración `el perro le ladra al gato` usando los parámetros estimados a partir del corpus  $q(w_i | w_{i-2}, w_{i-1})$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VUjDx0NrYbg"
   },
   "source": [
    "**Respuesta:**\n",
    "Tomemos $x_{-1}= x_{0} = * $, $x_{1} = el$ , $x_{2} = perro$ , $x_{3} = le$, $x_{4} = ladra$, $x_{5} = al$, $x_{6} = gato$, entonces la probabilidad asignada al corpus: \n",
    "\n",
    "$q(el\\;perro\\;le\\;ladra\\;al\\;gato) = q(el|* , * ) \\times q(perro|* , el) \\times q(le|el, perro) \\times q(ladra|perro, le) \\times q(al|le, ladra) \\times q(gato|ladra, al) \\times q(STOP|al, gato)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwNkPIXure0C"
   },
   "source": [
    "### Pregunta 2 (1 pt)\n",
    "Muestre cómo se calcularía  $q(w_{i} | w_{i-2}, w_{i-1})$  usando un modelo que interpola un modelo de lenguajes de trigramas, bigramas, y unigrama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeLZAd0Tr9ne"
   },
   "source": [
    "**Respuesta:**\n",
    "Supongamos que tenemos los hiper parametros $\\lambda_{1}\\;\\lambda_{2}\\;\\lambda_{3}$ ya calculados, el modelo que interpola al modelo del lenguaje usando trigramas, bigramas y unigramas estaria dado por:\n",
    "\n",
    "$q(w_{i}|w_{i-2},w_{i-1}) = \\lambda_{1} \\times q_{ML, 3}(w_{i}| w_{i-2}, w_{i-1}) + \\lambda_{2} \\times q_{ML,2}(w_{i}|w_{i-1}) + \\lambda_{3} \\times q_{ML,1}(w_{i})$\n",
    "\n",
    "Donde los $q_{ML,i}$ son los modelos no interpolas de trigramas, bigramas y unigramas respectivos para $ i \\in \\{3,2,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHqcRJ7Vr_8x"
   },
   "source": [
    "### Pregunta 3 (1 pt)\n",
    "¿Qué ventajas tiene el modelo interpolado sobre el modelo de trigramas simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F5R3Ji7sHjt"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Al aplicar el modelo de trigramas a un corpus, este produce un language model con mucha variabilidad pues es muy dependiente del corpus dado que los trigramas pueden estar o no en un corpus que quiera modelar con el language model producido\n",
    "\n",
    "En Machine Learning esto se conoce como el dilema de la sesgo-varianza, en que modelos con mas parametros y mas complejos tienen menos sesgo pero varian mucho dependiendo del corpus (como el caso de los trigramas). Pero modelos más simples con menos parametros tienen una menor varianza al aplicarlo en distintos corpus pero mayor sesgo (como es el caso de los unigramas)\n",
    "\n",
    "Para modelar de mejor manera el lenguaje se hace una interpolacion entre el modelo de trigramas simple con el de bigramas y unigramas, esto permite tener un buen balance entre sesgo y variabilidad de los corpus obteniendo asi un modelo que podemos generalizar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdmB-07ZKfaa"
   },
   "source": [
    "-----------------------\n",
    "## Naive Bayes (3 pts)\n",
    "En esta parte de la minitarea deberan programar Naive Bayes Multinomial usando Laplace Smothing. Las referencias las pueden encontrar en el material del curso y los videos del profesor.\n",
    "\n",
    "A continuacion se presentan un conjunto de documentos de training divididos en 2 categorias distintas. Ustedes deberan clasificar los documentos del test set usando Naive Bayes con Laplace Smothing.\n",
    "\n",
    "Por favor, documenten su codigo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algun lugar de su codigo hacen algo que creen que debe ser explicado, pongan comentarios, son bienvenidos. \n",
    "\n",
    "\n",
    "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplacenlas por sumas de logaritmos para prevenir errores de precision. Revisen la diapo 69 de las slides. \n",
    "\n",
    "NOTA: Sobre las `namedtuple`s. Es el tipo de los documentos. Son objetos inmutable que tienen dos atributos: `words` donde estan las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`. Son libres de implementar su solucion como quieran, si no les gusta este tipo de dato usen otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HLi8PxdV2VQX",
    "outputId": "efd50e50-3317-454b-c74f-222458249c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train documents:\n",
      "(document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
      " document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
      " document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
      " document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
      " document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
      " document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
      " document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
      " document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1))\n",
      "\n",
      "Test documents:\n",
      "(document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = (\n",
    "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
    "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
    "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
    "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
    "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
    "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
    "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
    "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
    ")\n",
    "print(\"Train documents:\")\n",
    "pprint(train_set)\n",
    "\n",
    "\n",
    "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n",
    "print(\"\\nTest documents:\")\n",
    "pprint(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0edu0E7LA3U9"
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "from typing import Dict, Tuple, Union, Set, List\n",
    "# Tu respuesta\n",
    "class NaiveBayesClassifier(object):\n",
    "    \"\"\"\n",
    "    Class that represents a Naive Bayes classifier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary: Set[str] = set()\n",
    "        self.document_classes: Set[int] = set()\n",
    "        self.class_probabilities: Dict[int, float] = dict()\n",
    "        self.word_probabilities: Dict[str, Dict] = dict()\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def vocabulary_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "    \n",
    "    \n",
    "    def __repr__(self) -> Dict[str, Union[Dict,Set]]:\n",
    "        return {\n",
    "            'vocabulary': self.vocabulary,\n",
    "            'document_classes': self.document_classes,\n",
    "            'class_probabilities': self.class_probabilities,\n",
    "            'word_probabilities': self.word_probabilities,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def _extract_vocabulary(self, training_set: Tuple) -> None:\n",
    "        \"\"\"\n",
    "        Extracts the vocabulary of the training set and store it on\n",
    "        the vocabulary field\n",
    "        \"\"\"\n",
    "        for document in training_set:\n",
    "            for word in document.words:\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary.add(word)\n",
    "                    \n",
    "                    \n",
    "    def _extract_classes(self, training_set: Tuple) -> None:\n",
    "        \"\"\"\n",
    "        Extracts all the classes of the training set and store them\n",
    "        on  document_classes field\n",
    "        \"\"\"\n",
    "        for document in training_set:\n",
    "            if document.class_ not in self.document_classes:\n",
    "                self.document_classes.add(document.class_)\n",
    "    \n",
    "    \n",
    "    def _calculate_class_probabilities(self, training_set: Tuple) -> None:\n",
    "        \"\"\"\n",
    "        Given a training set, clalculate all the class probabilities of that \n",
    "        training set and stores them on the class_probabilities field\n",
    "        \"\"\"\n",
    "        total_documents = len(training_set)\n",
    "        for document_class in self.document_classes:\n",
    "            total_document_class = len(list(filter(lambda x: x.class_ == document_class, training_set)))\n",
    "            self.class_probabilities[document_class] = log2(total_document_class / total_documents)\n",
    "            \n",
    "    def _calculate_word_probabilities(self, training_set: Tuple) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the word probabilities for the words on the training set on every class, using\n",
    "        laplace smoothing and store them on the word_probabilities field\n",
    "        \"\"\"\n",
    "        # Init the vocabulary probability dictionary of each word\n",
    "        for word in self.vocabulary:\n",
    "            self.word_probabilities[word] = dict()\n",
    "        \n",
    "        # Calculate the word probability on each class for each word\n",
    "        for document_class in self.document_classes:\n",
    "            class_documents = list(filter(lambda x: x.class_ == document_class, training_set))\n",
    "            class_documents_words = list(map(lambda x: x.words, class_documents))\n",
    "            total_words_in_class = self.vocabulary_size\n",
    "            for word in self.vocabulary: \n",
    "                document_ocurrences = [document.count(word) for document in class_documents_words]\n",
    "                total_words_in_class += sum(document_ocurrences)\n",
    "                \n",
    "            for word in self.vocabulary:\n",
    "                document_ocurrences = [document.count(word) for document in class_documents_words]\n",
    "                word_ocurrence = sum(document_ocurrences) + 1 #laplace smoothing\n",
    "                self.word_probabilities[word][document_class] = log2(word_ocurrence / total_words_in_class)\n",
    "                \n",
    "                \n",
    "    def train(self, training_set: Tuple[document]) -> None:\n",
    "        \"\"\"\n",
    "        Train the classifier with the given training set\n",
    "        \"\"\"\n",
    "        self._extract_vocabulary(training_set)\n",
    "        self._extract_classes(training_set)\n",
    "        self._calculate_class_probabilities(training_set)\n",
    "        self._calculate_word_probabilities(training_set)\n",
    "        \n",
    "    def calculate_probability_document_class(self, doc: document, possible_class: int) -> int:\n",
    "        \"\"\"\n",
    "        Given the document and the class, calculates the probability for that document being\n",
    "        generated for that class\n",
    "        \"\"\"\n",
    "        document_words = set()\n",
    "        for word in doc.words:\n",
    "            if word not in document_words:\n",
    "                document_words.add(word)\n",
    "        document_probability = 0\n",
    "        for word in document_words:\n",
    "            document_probability += self.word_probabilities[word][possible_class]\n",
    "        return self.class_probabilities[possible_class] + document_probability\n",
    "    \n",
    "    def predict_document(self, doc: document) -> int:\n",
    "        \"\"\"\n",
    "        Given a document, returns which class is more probable to generate that document\n",
    "        \"\"\"\n",
    "        document_class = 0\n",
    "        document_class_probability = self.calculate_probability_document_class(doc, 0)\n",
    "        for class_ in self.document_classes:\n",
    "            class_probability = self.calculate_probability_document_class(doc, class_)\n",
    "            if document_class_probability < class_probability:\n",
    "                document_class = class_\n",
    "                document_class_probability = class_probability\n",
    "        return document_class\n",
    "    \n",
    "    def predict(self, test_documents: Tuple[document]) -> List[document]:\n",
    "        \"\"\"\n",
    "        Classifies each document on the test_documents tuple \n",
    "        \"\"\"\n",
    "        test_documents_classified = list()\n",
    "        for doc in test_documents:\n",
    "            test_documents_classified.append(document(words=doc.words, class_=self.predict_document(doc)))\n",
    "        return test_documents_classified\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=1)]\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_set)\n",
    "pprint(classifier.predict(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "minitarea2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
